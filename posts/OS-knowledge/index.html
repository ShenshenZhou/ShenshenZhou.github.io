<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="天前"><meta name="hour-prompt" content="小时前"><meta name="minute-prompt" content="分钟前"><meta name="justnow-prompt" content="刚刚"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="OS相关知识整理" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="硬件结构" /><meta property="og:description" content="硬件结构" /><link rel="canonical" href="https://shenshenzhou.github.io/posts/OS-knowledge/" /><meta property="og:url" content="https://shenshenzhou.github.io/posts/OS-knowledge/" /><meta property="og:site_name" content="ShenshenZhou" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-22T11:48:55+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="OS相关知识整理" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="EbIRAK-yj0lEMVk1uQwtW66urJnY8yfKtT112zfnTfA" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-22T11:48:55+08:00","datePublished":"2022-07-22T11:48:55+08:00","description":"硬件结构","headline":"OS相关知识整理","mainEntityOfPage":{"@type":"WebPage","@id":"https://shenshenzhou.github.io/posts/OS-knowledge/"},"url":"https://shenshenzhou.github.io/posts/OS-knowledge/"}</script><title>OS相关知识整理 | ShenshenZhou</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ShenshenZhou"><meta name="application-name" content="ShenshenZhou"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://fastly.jsdelivr.net"><link rel="dns-prefetch" href="https://fastly.jsdelivr.net"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://fastly.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="zh-CN"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/images/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">ShenshenZhou</a></div><div class="site-subtitle font-italic">好好学习 天天向上</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/ShenshenZhou" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['15797870468','163.com'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-5" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>OS相关知识整理</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>OS相关知识整理</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> ShenshenZhou </span> 发表于 <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="2022-07-22, 11:48 +0800" >2022-07-22<i class="unloaded">2022-07-22T11:48:55+08:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="54386 字">302 分钟 阅读</span></div></div><div class="post-content"><h1 id="硬件结构">硬件结构</h1><h2 id="图灵机的基本组成工作方式">图灵机的基本组成、工作方式</h2><p><strong>图灵机的基本组成？</strong></p><ul><li>有一条「纸带」，纸带由一个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；<li>有一个「读写头」，读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子；<li>读写头上有一些部件，比如存储单元、控制单元以及运算单元： 1、存储单元用于存放数据； 2、控制单元用于识别字符是数据还是指令，以及控制程序的流程等； 3、运算单元用于执行运算指令；</ul><p><strong>图灵机的工作方式、工作原理、执行过程？</strong></p><ul><li>首先，读写头将数据先写入到纸带上的格子中；<li>然后，读取格子中的内容到存储设备中，这个存储设备称为图灵机的状态，控制单元用来识别字符是数据还是指令，如果是数据就存入状态中，碰到运算指令就会通知运算单元工作；<li>接着，运算单元读入数据，然后执行运算指令，并将结果存放到状态中；<li>最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头继续移动把结果 3写入到纸带的格子中。</ul><h2 id="冯诺依曼模型">冯诺依曼模型</h2><p><strong>冯诺依曼定义计算机基本结构为5个部分，其基本结构是？</strong></p><ul><li>运算器，在CPU中；<li>控制器，在CPU中；<li>存储器，即内存；<li>输入设备，键盘、鼠标等；<li>输出设备，显示器等。</ul><p><strong>内存？</strong></p><p>程序和数据存储在内存中，存储的区域是线性的。计算机最小的存储单位是字节byte，1 byte=8 bit,每一个字节都对应一个内存地址。内存的地址是从 0 开始编号的，然后自增排列，最后一个地址为内存总字节数 - 1，所以内存读写任何一个数据的速度都是一样的。</p><p><strong>中央处理器CPU？</strong></p><p>32位、64 位 CPU 最主要区别在于一次能计算多少字节数据，之所以这样设计，是为了能计算更大的数值：</p><ul><li>32 位 CPU 一次可以计算 4 个字节。<li>64 位 CPU 一次可以计算 8 个字节。</ul><p>CPU 内部常见组件：</p><ul><li>寄存器，不同种类寄存器的功能不尽相同。<li>控制单元，负责控制 CPU 工作。<li>逻辑运算单元，负责计算。</ul><p>常见的寄存器种类：</p><ul><li>通用寄存器，用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。<li>程序计数器，用来存储 CPU 要执行下一条指令「所在的内存地址」，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令的地址。<li>指令寄存器，用来存放程序计数器指向的指令，也就是指令本身，指令被执行完成之前都存储在这里。</ul><p><strong>总线？</strong></p><p>总线的作用：</p><blockquote><p>用于 CPU 和内存以及其他设备之间的通信。</p></blockquote><p>总线分类：</p><ul><li>地址总线，用于指定 CPU 将要操作的内存地址；<li>数据总线，用于读写内存的数据；<li>控制总线，用于发送和接收信号，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；</ul><p>CPU使用总线读写内存数据的流程：</p><ul><li>首先要通过「地址总线」来指定内存的地址；<li>然后通过「控制总线」控制是读或写命令；<li>最后通过「数据总线」来传输数据；</ul><h2 id="线路位宽与-cpu-位宽">线路位宽与 CPU 位宽</h2><p>线路位宽：</p><blockquote><p>就是线路的条数，数据通过电压高低来表示二进制数据，一条线路每次只能传送1位数据，即0或1，称为串行通信，想要一次传送一些数据，就需要多条线路并行通信。比如 CPU想要操作4 G大的内存，那么就需要 32 条地址总线，因为 <code class="language-plaintext highlighter-rouge">2 ^ 32 = 4G</code>。</p></blockquote><p>CPU位宽：</p><blockquote><p>就是一次能够处理的数据位数。</p></blockquote><h2 id="程序执行的基本过程">程序执行的基本过程</h2><p><strong>CPU 执行程序的过程？</strong></p><ul><li>第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。<li>第二步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；<li>第三步，CPU 执行完指令后，「程序计数器」的值自增，表示指向下一条指令。这个自增的大小，由 CPU 的位宽决定，比如 32 位的 CPU，指令是 4 个字节，需要 4 个内存地址存放，因此「程序计数器」的值会自增 4；</ul><p><strong>CPU 的指令周期？</strong></p><blockquote><p>CPU 从程序计数器读取指令、到执行、再到下一条指令，这个不断循环的过程即为指令周期。</p></blockquote><ul><li>CPU 通过程序计数器读取对应内存地址的指令，这个部分称为 Fetch（取得指令）；<li>CPU 对指令进行解码，这个部分称为 Decode（指令译码）；<li>CPU 执行指令，这个部分称为 Execution（执行指令）；<li>CPU 将计算结果存回寄存器或者将寄存器的值存入内存，这个部分称为 Store（数据回写）；</ul><p>事实上，不同的阶段是由计算机中的不同组件完成：</p><ul><li>取指令的阶段，我们的指令是存放在存储器里的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由控制器操作的；<li>指令的译码过程，也是由控制器进行的；<li>指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由算术逻辑单元操作的，也就是由运算器处理的。但是如果是一个简单的无条件地址跳转，则是直接在控制器里面完成的，不需要用到运算器。</ul><p><strong>指令的类型？</strong></p><p>指令从功能角度划分，可以分为 5 大类：</p><ul><li>数据传输类型的指令，比如 <code class="language-plaintext highlighter-rouge">store/load</code> 是寄存器与内存间数据传输的指令，<code class="language-plaintext highlighter-rouge">mov</code> 是将一个内存地址的数据移动到另一个内存地址的指令；<li>运算类型的指令，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；<li>跳转类型的指令，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 <code class="language-plaintext highlighter-rouge">if-else</code>、<code class="language-plaintext highlighter-rouge">swtich-case</code>、函数调用等。<li>信号类型的指令，比如发生中断的指令 <code class="language-plaintext highlighter-rouge">trap</code>；<li>闲置类型的指令，比如指令 <code class="language-plaintext highlighter-rouge">nop</code>，执行后 CPU 会空转一个周期；</ul><p><strong>如何让程序跑的更快？</strong></p><p>程序的CPU执行时间=CPU时钟周期数*时钟周期时间。</p><p>CPU是时钟周期数=指令数*每条指令的平均时钟周期数（Cycles Per Instruction，简称 <code class="language-plaintext highlighter-rouge">CPI</code>）。</p><p>所以，需要优化一下三方面：</p><ul><li>指令数，表示执行程序所需要多少条指令，以及哪些指令。这个层面是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示方式。<li>每条指令的平均时钟周期数 CPI，表示一条指令需要多少个时钟周期数，现代大多数 CPU 通过流水线技术（Pipeline），让一条指令需要的 CPU 时钟周期数尽可能的少；<li>时钟周期时间，表示计算机主频，取决于计算机硬件。有的 CPU 支持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU 工作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压力就会越大，CPU 会很容易奔溃。</ul><p><strong>64 位相比 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能一定比 32 位 CPU 高很多吗？</strong></p><p>64 位相比 32 位 CPU 的优势主要体现在两个方面：</p><ul><li>64 位 CPU 可以一次计算超过 32 位的数字，而 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进行计算，效率就没那么高，但是大部分应用程序很少会计算那么大的数字，所以只有运算大数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不大。<li>64 位 CPU 可以寻址更大的内存空间，32 位 CPU 最大的寻址地址是 4 G，即使你加了 8 G 大小的内存，也还是只能寻址到 4 G，而 64 位 CPU 最大寻址地址是 <code class="language-plaintext highlighter-rouge">2^64</code>，远超于 32 位 CPU 最大寻址地址的 <code class="language-plaintext highlighter-rouge">2^32</code>。</ul><p><strong>你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运行在 64 位的电脑上吗？64 位的操作系统可以运行在 32 位的电脑上吗？如果不行，原因是什么？</strong></p><p>64 位和、32 位软件，代表指令是 64 位还是 32 位的：</p><ul><li>如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令；<li>操作系统其实也是一种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，比如 64 位操作系统，指令也就是 64 位，因此不能装在 32 位机器上。</ul><h2 id="存储器的层次结构">存储器的层次结构</h2><p><strong>速度快的存储器的容量都比较小？</strong></p><blockquote><p>对于存储器，它的速度越快，能耗会越高，而且材料的成本也是越贵的。</p></blockquote><p><strong>存储器的层次结构？</strong></p><ul><li>寄存器，CPU中；<li>缓存（cache），L1、L2、L3，CPU中；<li>内存；<li>固态硬盘（SSD）、机械硬盘（HDD）。</ul><p><strong>寄存器？</strong></p><p>寄存器是最靠近 CPU 的控制单元和逻辑计算单元的存储器，使用材料速度是最快的，因此价格也是最贵的。</p><p>寄存器的数量通常在几十到几百之间，每个寄存器可以用来存储一定的字节的数据，比如：</p><ul><li>32 位 CPU 中大多数寄存器可以存储 <code class="language-plaintext highlighter-rouge">4</code> 个字节；<li>64 位 CPU 中大多数寄存器可以存储 <code class="language-plaintext highlighter-rouge">8</code> 个字节</ul><p>寄存器的访问速度非常快，一般要求在半个 CPU 时钟周期内完成读写。CPU 时钟周期跟 CPU 主频息息相关。</p><p><strong>CPU缓存？</strong></p><p>CPU Cache 用的是一种叫 SRAM（Static Random-Access Memory，静态随机存储器） 的芯片。</p><p>SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了。在SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管。</p><p>CPU 的高速缓存，通常可以分为 L1、L2、L3 这样的三层高速缓存，也称为一级缓存、二次缓存、三次缓存。</p><p><strong>L1 高速缓存？</strong></p><p>L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 <code class="language-plaintext highlighter-rouge">2~4</code> 个时钟周期，而大小在几十 KB 到几百 KB 不等。</p><p>每个 CPU 核心都有一块属于自己的 L1 高速缓存，指令和数据在 L1 是分开存放的，所以 L1 高速缓存通常分成指令缓存和数据缓存。</p><p><strong>L2高速缓存？</strong></p><p>L2 高速缓存同样每个 CPU 核心都有，但是 L2 高速缓存位置比 L1 高速缓存距离 CPU 核心 更远，它大小比 L1 高速缓存更大，CPU 型号不同大小也就不同，通常大小在几百 KB 到几 MB 不等，访问速度则更慢，速度在 <code class="language-plaintext highlighter-rouge">10~20</code> 个时钟周期。</p><p><strong>L3高速缓存？</strong></p><p>L3 高速缓存通常是多个 CPU 核心共用的，位置比 L2 高速缓存距离 CPU 核心 更远，大小也会更大些，通常大小在几 MB 到几十 MB 不等，具体值根据 CPU 型号而定。访问速度相对也比较慢一些，访问速度在 <code class="language-plaintext highlighter-rouge">20~60</code>个时钟周期。</p><p><strong>内存？</strong></p><p>内存用的芯片和 CPU Cache 有所不同，它使用的是一种叫作 DRAM （Dynamic Random Access Memory，动态随机存取存储器） 的芯片。</p><p>相比 SRAM，DRAM 的密度更高，功耗更低，有更大的容量，而且造价比 SRAM 芯片便宜很多。</p><p>DRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。断电后仍然会丢失数据。</p><p>DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问的速度会更慢，内存速度大概在 <code class="language-plaintext highlighter-rouge">200~300</code> 个 时钟周期之间。</p><p><strong>SSD/HDD 硬盘？</strong></p><p>固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失，但是读写速度比内存慢。</p><p>械硬盘HDD，它是通过物理读写的方式来访问数据，因此它访问速度是非常慢的。</p><p><strong>存储器的层次关系</strong>：</p><p>存储器层次结构是分级的，每个存储器只和相邻的一层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更高，也正因为成本太高，所以 CPU 内部的寄存器、L1\L2\L3 Cache 只好用较小的容量，相反内存、硬盘则可用更大的容量，</p><p>另外，当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU 就会查询 L1 高速缓存，如果 L1 没有，则查询 L2 高速缓存，L2 还是没有的话就查询 L3 高速缓存，L3 依然没有的话，才去内存中取数据。</p><h2 id="cpu-cache-的数据结构和读取过程是什么样的">CPU Cache 的数据结构和读取过程是什么样的？</h2><p><strong>缓存块（cache line）？</strong></p><blockquote><p>CPU缓存的数据是从内存中读取过来的，它是按块读取数据的，而不是按照单个元素来读取数据，因此称之为缓存块。</p></blockquote><p><strong>内存块（block）?</strong></p><blockquote><p>CPU访问内存数据时，也是按块读取，称之为内存块，读取的时候我们要拿到数据所在内存块的地址。</p></blockquote><p><strong>CPU如何找到 Cache 对应的数据呢？</strong></p><p>直接映射：通过取模运算，将内存块的地址始终映射在一个CPU缓存块地址。</p><p><strong>CPU缓存数据结构？</strong></p><ul><li>组标记：记录缓存块中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。<li>索引：通过内存地址索引，找出对应的缓存地址。<li>有效位：它是用来标记对应的缓存中的数据是否是有效的，如果有效位是 0，无论 缓存中是否有数据，CPU 都会直接访问内存，重新加载数据。<li>数据块：从内存加载过来的实际存放数据。<li>偏移量：CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个字。那通过偏移量在对应的缓存数据块中找到所需的字。</ul><p><strong>CPU访问缓存步骤？</strong></p><ol><li>根据内存地址中索引信息，计算在 CPU Cahe 中的索引，也就是找出对应的 CPU line 的地址；<li>找到对应 CPU Line 后，判断 CPU Line 中的有效位，确认 CPU Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；<li>对比内存地址中组标记和 CPU Line 中的组标记，确认 CPU Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；<li>根据内存地址中偏移量信息，从 CPU Line 的数据块中，读取对应的字。</ol><h2 id="如何写出让-cpu-跑得更快的代码">如何写出让 CPU 跑得更快的代码？</h2><p>访问的数据在 CPU Cache 中的话，意味着<strong>缓存命中</strong>，缓存命中率越高的话，代码的性能就会越好，CPU 跑的越快。</p><ul><li>对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；<li>对于指令缓存，有规律的条件分支语句能够让 CPU 的分支预测器发挥作用，进一步提高执行的效率</ul><p><strong>如何提升多核 CPU 的缓存命中率？</strong></p><p>现代CPU 都是多核心的，线程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，如果一个线程在不同核心来回切换，各个核心的缓存命中率就会受到影响，相反如果线程都在同一个核心上执行，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。</p><p>当有多个同时执行「计算密集型」的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把线程绑定在某一个 CPU 核心上，这样性能可以得到非常可观的提升。</p><h2 id="cpu-缓存一致性">CPU 缓存一致性</h2><h3 id="cpu-cache-的数据写入">CPU Cache 的数据写入</h3><p>CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png" alt="img" /></p><p>CPU读取数据的时候，我们希望高命中率。写数据之后，cache和内存数据就不一致了，这时候就需要将cache中的数据同步到内存里。下面是两种数据写入方式：</p><ul><li>写直达（<em>Write Through</em>）<li>写回（<em>Write Back</em>）</ul><p>==写直达==</p><p>保持内存与 Cache 一致性最简单的方式是，<strong>把数据同时写入内存和 Cache 中</strong>，这种方法称为<strong>写直达</strong>。在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：</p><ul><li>如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；<li>如果数据没有在 Cache 里面，就直接把数据更新到内存里面。</ul><p>写直达很简单，但是每次写操作都要写回到内存，会影响性能。</p><p>==写回==</p><p>在写回机制中，<strong>当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中</strong>，减少了数据写回内存的频率，这样便可以提高系统的性能。</p><p>在把数据写入到 Cache 的时候，只有在缓存不命中，同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下，才会将数据写到内存中，而在缓存命中的情况下，则在写入后 Cache 后，只需把该数据对应的 Cache Block 标记为脏即可，而不用写到内存里。</p><h3 id="缓存一致性问题">缓存一致性问题</h3><p>现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核心各自独有的，那么会带来多核心的缓存一致性问题，具体就是某个核心修改了数据，但是还没有写入到内存，而另一个核心就读取了数据，导致缓存数据不一致。</p><p>要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据：</p><ul><li>第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为<strong>写传播</strong><li>第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为<strong>事务的串形化</strong></ul><h3 id="总线嗅探">总线嗅探</h3><p>写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常见实现的方式是<strong>总线嗅探</strong>。总线嗅探就是总线将事务广播给其它所有核心，每个核心监听总线上的广播时间，然后根据接收到的时间进行数据操作。</p><p>很简单，缺点是CPU需要时刻监听总线上的一切活动，只要有数据操作，总线都要发出广播时间，会加重CPU和总线的负担。而且总线嗅探不能保证事务的串行化。</p><p>MESI 协议基于总线嗅探机制实现了事务串形化，也用状态机机制降低了总线带宽压力，可以做到 CPU 缓存一致性。</p><h3 id="mesi-协议">MESI 协议</h3><p>MESI 协议其实是 4 个状态单词的开头字母缩写，分别是：</p><ul><li><em>Modified</em>，已修改<li><em>Exclusive</em>，独占<li><em>Shared</em>，共享<li><em>Invalidated</em>，已失效</ul><p>这四个状态来标记 Cache Line 四个不同的状态。</p><p>「已修改」状态就是脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存里。而「已失效」状态，表示的是这个 Cache Block 里的数据已经失效了，不可以读取该状态的数据。</p><p>「独占」和「共享」状态都代表 Cache Block 里的数据是干净的，也就是说，这个时候 Cache Block 里的数据和内存里面的数据是一致性的。「独占」和「共享」的差别在于，独占状态的时候，数据只存储在一个 CPU 核心的 Cache 里，而其他 CPU 核心的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接自由地写入，而不需要通知其他 CPU 核心，因为只有独占的cache有这个数据，就不存在缓存一致性的问题了，所以可以随便操作该数据。</p><p>在「独占」状态下的数据，如果有其他核心从内存读取了相同的数据到各自的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。「共享」状态代表着相同的数据在多个 CPU 核心的 Cache 里都有，所以当我们要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后再更新当前 Cache 里面的数据。</p><p>整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心，可以在一定程度上减少总线带宽压力。</p><h2 id="什么是软中断">什么是软中断？</h2><p>==中断是什么？==</p><p><strong>中断</strong>是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。</p><p>==什么是软中断？==</p><p>中断请求的处理程序应该要短且快，这样才能减少对正常进程运行调度的影响，而且中断处理程序可能会暂时关闭中断，所以如果中断处理程序执行时间过长，可能在还未执行完就丢失当前其他设备的中断请求。</p><p>Linux 系统为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是<strong>「上半部和下半部分」</strong>：</p><ul><li><strong>上半部用来快速处理中断</strong>，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或时间敏感的事情。<li><strong>下半部用来延迟处理上半部未完成的工作</strong>，一般以「内核线程」的方式运行。</ul><p>所以，中断处理程序的上部分和下半部可以理解为：</p><ul><li><strong>上半部直接处理硬件请求，也就是硬中断</strong>，主要是负责耗时短的工作，特点是快速执行；<li><strong>下半部是由内核触发，也就说软中断</strong>，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行。</ul><p>还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程</p><p>==怎么查看软中断？==</p><p>在 Linux 系统里，我们可以通过查看 <code class="language-plaintext highlighter-rouge">cat /proc/softirqs</code> 的 内容来知晓「软中断」的运行情况，软中断是以内核线程的方式执行的，也可以用 <code class="language-plaintext highlighter-rouge">ps</code> 命令查看到，以及 <code class="language-plaintext highlighter-rouge">cat /proc/interrupts</code> 的 内容来知晓「硬中断」的运行情况。</p><h2 id="为什么-01--02-不等于-03-">为什么 0.1 + 0.2 不等于 0.3 ？</h2><p>==为什么负数要用补码表示？==</p><p>十进制转换为二进制是<strong>除二取余法</strong>，二进制在计算机中最高位为符号位（正数为0，负数为1），剩下的则是表示数据位。负数在计算机中以补码形式表示，补码就是把整数的二进制全部取反再加1。</p><p>因为用了补码的表示方式，对于负数的加减法操作，就和正数加减法操作一样了，运算过程简单。</p><p>==十进制小数与二进制的转换==</p><p><strong>乘 2 取整法</strong>：将十进制中的小数部分乘以 2 作为二进制的一位，然后继续取小数部分乘以 2 作为下一位，直到不存在小数为止。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%8D%81%E8%BF%9B%E5%88%B6%E5%B0%8F%E6%95%B0%E8%BD%AC%E4%BA%8C%E8%BF%9B%E5%88%B6.png" alt="img" /></p><p>但是并不是所有的小数都可以用二进制表示，比如0.1转换为二进制是无限循环的，由于计算机的资源是有限的，所以没有办法用二进制精确表示0.1，只能用近似的方法，所以会造成精度确实的情况。</p><p>二进制小数转十进制时，需要注意小数点后面的指数幂是<strong>负数</strong>。</p><p>==计算机是怎么存小数的？==</p><p>计算机存储小数时用浮点数，并且使用科学计数法。比如float（单精度浮点数，32位浮点数）的有效数字时7-8位，double（双精度浮点数，64位浮点数）有效数字是15-16位。</p><p>==0.1 + 0.2 == 0.3 ?==</p><p>由于计算机的资源是有限的，所以并不是所有的小数都可以用完整的二进制来表示，计算机智能采用近似数的方式来保存，那两个近似数相加，得到的也是一个近似数，所以0.1+0.2不等于0.3。</p><h1 id="操作系统结构">操作系统结构</h1><h2 id="linux-内核-vs-windows-内核">Linux 内核 vs Windows 内核</h2><p>==什么是内核呢？==</p><p>计算机是由各种外部硬件设备组成的，如果每个应用都要和这些硬件设备对接通信协议那太麻烦了，所以<strong>让内核作为应用连接硬件设备的桥梁</strong>，应用程序只需关心与内核交互，不用关心硬件的细节。</p><p>==内核有哪些能力呢？==</p><p>现代操作系统，内核一般会提供 4 个基本能力：</p><ul><li>管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力；<li>管理内存，决定内存的分配和回收，也就是内存管理的能力；<li>管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；<li>提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。</ul><p>==内核是怎么工作的？==</p><p>大多数操作系统，把内存分成了两个区域：</p><ul><li>内核空间，这个内存空间只有内核程序可以访问；<li>用户空间，这个内存空间专门给应用程序使用；</ul><p>用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，就是在<strong>用户态</strong>执行，而当程序使内核空间时，就是在<strong>内核态</strong>执行。</p><p>应用程序如果需要进入内核空间，就需要系统调用，当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作。</p><p>==常见内核架构==</p><ul><li>宏内核，包含多个模块，整个内核像一个完整的程序；<li>微内核，有一个最小版本的内核，一些模块和服务则由用户态管理；<li>混合内核，是宏内核和微内核的结合体，内核中抽象出了微内核的概念，也就是内核中会有一个小型的内核，其他模块就在这个基础上搭建，整个内核是个完整的程序；</ul><p>==Linux和Windows内核的区别==</p><p>Linux 内核设计的理念主要有这几个点：</p><ul><li><em>MultiTask</em>，多任务。多任务意味着可以有多个任务同时执行，可以是并发或并行：<ul><li>对于单核 CPU 时，可以让每个任务执行一小段时间，时间到就切换另外一个任务，从宏观角度看，一段时间内执行了多个任务，这被称为并发。<li>对于多核 CPU 时，多个任务可以同时被不同核心的 CPU 同时执行，这被称为并行。</ul><li><em>SMP</em>，对称多处理。代表着每个 CPU 的地位是相等的，对资源的使用权限也是相同的，多个 CPU 共享同一个内存，每个 CPU 都可以访问完整的内存和硬件资源。这个特点决定了 Linux 操作系统不会有某个 CPU 单独服务应用程序或内核程序，而是每个程序都可以被分配到任意一个 CPU 上被执行。<li><em>ELF</em>，可执行文件链接格式。<li><em>Monolithic Kernel</em>，宏内核</ul><p>Windows 和 Linux 一样，同样支持 MultiTask 和 SMP，但Window 是混合型内核，它的可执行文件格式为PE（可移植执行文件）。</p><h1 id="内存管理">内存管理</h1><h2 id="为什么要有虚拟内存">为什么要有虚拟内存？</h2><h3 id="虚拟内存">虚拟内存</h3><p>如果CPU直接操作内存的物理地址（单片机就是这样），那想在内存中同时运行两个程序是行不通的。因为如果程序在某个位置写入了一个新的值，将会擦掉另一个程序放在相同位置上的所有内容，程序会立即崩溃。</p><p>解决方法：操作系统为每个进程分配独立的虚拟地址，然后将不同进程的虚拟地址和不同内存的物理地址建立映射关系。如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样就保证不同的进程运行时写入的是不同的物理地址，也就不会冲突了。因此有以下两种地址概念：</p><ul><li>程序所使用的内存地址叫做<strong>虚拟内存地址</strong>（<em>Virtual Memory Address</em>）<li>实际存在硬件里面的空间地址叫<strong>物理内存地址</strong>（<em>Physical Memory Address</em>）。</ul><p>虚拟地址是通过 <strong>CPU 芯片中的内存管理单元（MMU）</strong>的映射关系来转换变成物理地址的。</p><p>操作系统管理虚拟地址和物理地址之间的估值关系主要有两种方式：内存分段和内存分页。</p><h3 id="内存分段">内存分段</h3><p>程序是由若干个逻辑分段组成的，比如由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段的形式把这些段分离出来。</p><p>==分段机制下，虚拟地址和物理地址是如何映射的？==</p><p>分段机制下的虚拟地址由两部分组成，<strong>段选择因子</strong>和<strong>段内偏移量</strong>。</p><ul><li>段选择因子就保存在段寄存器里面。段选择因子里面最重要的是<strong>段号</strong>，用作段表的索引。<strong>段表</strong>里面保存的是这个<strong>段的基地址、段的界限和特权等级</strong>等。<li>虚拟地址中的<strong>段内偏移量</strong>应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。</ul><p>虚拟地址是通过<strong>段表</strong>与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段都有一个段号，分别对应段表中的一个项，在相应的项中找到段的基地址，再加上偏移量，就能找到物理内存中的地址。</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png" alt="img" /></p><p>分段的方法能够产生连续的内存空间，但它也有一些缺点：</p><ul><li>第一个就是<strong>内存碎片</strong>的问题。<li>第二个就是<strong>内存交换的效率低</strong>的问题。</ul><p>==分段为什么会产生内存碎片的问题？==</p><p>内存碎片问题有两种：</p><ul><li>外部内存碎片，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载；<li>内部内存碎片，程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费；</ul><p>解决内存碎片问题的方法就是<strong>内存交换</strong>。内存交换就是把原有程序占用的内存先写到硬盘上，然后再从硬盘上读回来到内存里，这样就能空缺处连续的内存空间，然后让新程序装载进去。比如Linux系统里面的swap空间，就是从硬盘划分出来的，用于内存和硬盘的空间交换。</p><p>==分段为什么会导致内存交换效率低的问题？==</p><p>对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 <code class="language-plaintext highlighter-rouge">Swap</code> 内存区域，这个过程会产生性能瓶颈。因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上然后再写回内存，这个操作效率很低。</p><h3 id="内存分页">内存分页</h3><p>为了解决内存分段的内存碎片和内存交换效率低的问题，出现了内存分页。</p><p><strong>分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小</strong>。这样一个连续并且尺寸固定的内存空间，就是<strong>页（Page）</strong>，在 Linux 下，每一页的大小为 <code class="language-plaintext highlighter-rouge">4KB</code>。</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png" alt="img" /></p><p>虚拟地址与物理地址之间通过<strong>页表</strong>来映射，页表是存储在内存里的，<strong>内存管理单元 （MMU）</strong>就做将虚拟内存地址转换成物理地址的工作。当进程访问的虚拟地址在页表中查不到时，系统会产生一个<strong>缺页异常</strong>，然后就会进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。</p><p>==分页是怎么解决分段的内存碎片、内存交换效率低的问题？==</p><p>采用分页之后，内存空间都是预先划分好的，释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。这样就可以很好的避免分段的内存碎片问题。</p><p>如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为<strong>换出</strong>（<em>Swap Out</em>）。一旦需要的时候，再加载进来，称为<strong>换入</strong>（<em>Swap In</em>）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，<strong>内存交换的效率就相对比较高。</strong></p><p>更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存页之间的映射之后，不直接把页加载到物理内存中，而是<strong>只有在程序运行中需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存中去。</strong></p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png" alt="img" /></p><p>==分页机制下，虚拟地址和物理地址是如何映射的？==</p><p>在分页机制下，虚拟地址分为两部分，<strong>页号</strong>和<strong>页内偏移</strong>。页号作为页表的索引，页表包含物理页每页所在<strong>物理内存的基地址</strong>，这个基地址与页内偏移的组合就形成了物理内存地址：</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png" alt="img" /></p><p>也就是说，内存地址转换包括三个步骤：</p><ul><li>把虚拟内存地址，切分成页号和偏移量；<li>根据页号，从页表里面，查询对应的物理页号；<li>直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。</ul><p>==简单的分页有什么缺陷吗？==</p><p>如果只是简单的分页，因为操作系统同时运行非常多的进程，会导致页表占用的内存非常大。</p><p>比如在32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 <code class="language-plaintext highlighter-rouge">4MB</code> 的内存来存储页表。4MB 的页表并不是很大，但是每个进程都是有自己的虚拟地址空间，都有自己的页表，如果是100个进程的话，就需要400MB的内存来存储页表，这样页表占用的内存就太大了。</p><p>为了解决这个问题，才用了多级页表的方法。</p><h4 id="多级页表">多级页表</h4><p>对页表进行分级，比如一级页表的页表项中存放的是一级页表号和对应的二级页表地址，而二级页表中存放的是二级页表号和对应的物理页号。<strong>多级页表是利用了局部性原理</strong>，如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即只在需要时才创建二级页表，这样就可以节约很多内存。</p><p>做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= <code class="language-plaintext highlighter-rouge">0.804MB</code>，这对比单级页表的 <code class="language-plaintext highlighter-rouge">4MB</code> 来说是一个巨大的节约！</p><h4 id="tlb">TLB</h4><p>多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然降低了地址转换的速度，带来的时间开销更大。</p><p>程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。我们可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，CPU中有一个专门存放程序最常访问页表项的cache，即TLB，也称为页表缓存、转址旁路缓存、快表等。</p><p>在 CPU 芯片里面，封装了内存管理单元（MMU）芯片，它用来完成地址转换和 TLB 的访问与交互。有了 TLB 后，CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。</p><h3 id="段页式内存管理">段页式内存管理</h3><p>内存分段可以和内存分页组合使用，即段页式内存管理。实现方式如下：</p><ul><li>先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；<li>接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；</ul><p>这样，地址结构就由<strong>段号、段内页号和页内位移</strong>三部分组成。用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png" alt="img" /></p><p>段页式地址变换中要得到物理地址须经过三次内存访问：</p><ul><li>第一次访问段表，得到页表起始地址；<li>第二次访问页表，得到物理页号；<li>第三次将物理页号与页内位移组合，得到物理地址。</ul><p>可用软、硬件相结合的方法实现段页式内存管理，虽然增加了硬件成本和系统开销，但提高了内存的利用率。</p><h3 id="linux-内存管理">Linux 内存管理</h3><p><strong>Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制</strong>。因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。</p><p>但是<strong>Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下）</strong>，也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。</p><h2 id="malloc-是如何分配内存的">malloc 是如何分配内存的？</h2><h3 id="linux-进程的内存分布长什么样">Linux 进程的内存分布长什么样？</h3><p>在 Linux 操作系统中，虚拟地址空间的内部又被分为<strong>内核空间和用户空间</strong>两部分，不同位数的系统，地址空间的范围也不同：</p><ul><li><code class="language-plaintext highlighter-rouge">32</code> 位系统的内核空间占用 <code class="language-plaintext highlighter-rouge">1G</code>，位于最高处，剩下的 <code class="language-plaintext highlighter-rouge">3G</code> 是用户空间；<li><code class="language-plaintext highlighter-rouge">64</code> 位系统的内核空间和用户空间都是 <code class="language-plaintext highlighter-rouge">128T</code>，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。</ul><p>内核空间与用户区间的区别在于：</p><ul><li>进程在用户态时，只能访问用户空间内存；<li>只有进入内核态后，才可以访问内核空间的内存；</ul><p>用户空间分布情况（32位系统为例）：</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/img_convert/7b5b6b3728acde8df019350df3cb85c1.png" alt="图片" /></p><ul><li>程序文件段，包括二进制可执行代码；<li>已初始化数据段，包括静态常量；<li>未初始化数据段，包括未初始化的静态变量；<li>堆段，包括动态分配的内存，从低地址开始向上增长；<li>文件映射段，包括动态库、共享内存等；<li>栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 <code class="language-plaintext highlighter-rouge">8 MB</code>。当然系统也提供了参数，以便我们自定义大小；</ul><p>在这 6 个内存段中，堆和文件映射段的内存是动态分配的。可以使用C 标准库的 <code class="language-plaintext highlighter-rouge">malloc()</code>进行动态分配。</p><h3 id="malloc-是如何分配内存的-1">malloc 是如何分配内存的？</h3><p>malloc() 并不是系统调用，而是 C 库函数，malloc 申请内存的时候，会有两种方式向操作系统申请堆内存：</p><ul><li>方式一：通过 brk() 系统调用从堆分配内存<li>方式二：通过 mmap() 系统调用在文件映射区域分配内存；</ul><p>方式一实现的方式很简单，就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。</p><p>方式二通过 mmap() 系统调用将一个文件或者其它对象映射进内存。</p><p>==什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？==</p><p>malloc() 源码里默认定义了一个阈值：</p><ul><li>如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；<li>如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；</ul><h3 id="malloc-分配的是物理内存吗">malloc() 分配的是物理内存吗？</h3><p>不是的，<strong>malloc() 分配的是虚拟内存</strong>。如果分配后的虚拟内存没有被访问的话，是不会将虚拟内存映射到物理内存，这样就不会占用物理内存了。只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。</p><h3 id="malloc1-会分配多大的虚拟内存">malloc(1) 会分配多大的虚拟内存？</h3><p>malloc() 在分配内存的时候，并不是按用户预期申请的字节数来分配内存空间大小，而是<strong>会预分配更大的空间作为内存池</strong>。</p><h3 id="free-释放内存会归还给操作系统吗">free 释放内存，会归还给操作系统吗？</h3><p>free 内存后堆内存还存在，是针对 malloc 通过 brk() 方式申请的内存的情况，将释放的内存先缓存着放进malloc的内存池中，当进程再次申请内存时就可以直接复用，这样速度更快。如果 malloc 通过 mmap 方式申请的内存，free 释放内存后就会归还给操作系统。</p><h3 id="为什么不全部使用-mmap-来分配内存">为什么不全部使用 mmap 来分配内存</h3><p>因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。所以频繁通过 mmap 分配内存的话，不仅每次都要执行系统调用，而系统调用会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大。</p><p>为了改进这两个问题，malloc 通过 brk() 系统调用申请堆空间内存的时候，会直接预分配更大的内存来作为内存池，当内存释放的时候，就先缓存在内存池中。<strong>等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗</strong>。</p><h3 id="既然-brk-那么牛逼为什么不全部使用-brk-来分配">既然 brk 那么牛逼，为什么不全部使用 brk 来分配？</h3><p>因为随着系统频繁地 malloc 和 free ，堆内将产生越来越多不可用的小块内存碎片，会导致内存泄露问题。</p><p>所以，malloc 充分考虑了sbrk和mmap的优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。</p><h3 id="free-函数只传入一个内存地址为什么能知道要释放多大的内存">free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？</h3><p>因为malloc中有一部分空间保存了分配的内存块的描述信息，比如内存块的大小。这样执行free()函数时，free会对传入进来的内存地址偏移一定大小，然后从描述信息中分析出当前内存块的大小，自然就知道要释放多少内存了。</p><h2 id="内存满了会发生什么">内存满了，会发生什么？</h2><h3 id="内存分配的过程是怎样的">内存分配的过程是怎样的？</h3><p>应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生<strong>缺页中断</strong>，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。如果没有空闲的物理内存，那么内核就会开始进行<strong>回收内存</strong>的工作，回收的方式主要是两种：直接内存回收和后台内存回收。</p><ul><li><strong>后台内存回收</strong>（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程<strong>异步</strong>的，不会阻塞进程的执行。<li><strong>直接内存回收</strong>（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是<strong>同步</strong>的，会阻塞进程的执行。</ul><p>如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——<strong>触发 OOM （Out of Memory）机制</strong>。OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。</p><h3 id="哪些内存可以被回收">哪些内存可以被回收？</h3><p>主要有两类内存可以被回收，而且它们的回收方式也不同：</p><ul><li><strong>文件页</strong>（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，<strong>回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存</strong>。<li><strong>匿名页</strong>（Anonymous Page）：这部分内存没有实际载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们<strong>回收的方式是通过 Linux 的 Swap 机制</strong>，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。</ul><p>文件页和匿名页的回收都是基于 <strong>LRU 算法</strong>，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：</p><ul><li><strong>active_list</strong> 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；<li><strong>inactive_list</strong> 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；</ul><p>越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统优先回收不活跃的内存。</p><h3 id="回收内存带来的性能影响">回收内存带来的性能影响</h3><p>回收内存的操作基本都会发生磁盘 I/O ，如果回收内存的操作很频繁，磁盘 I/O的 次数也会很频繁，这个过程势必会影响系统的性能。可以使用以下方式优化：</p><ul><li>==调整文件页和匿名页的回收倾向==</ul><p>从文件页和匿名页的回收操作来看，文件页的回收操作对系统的影响相比匿名页的回收操作会少一点，因为文件页对于干净页回收是不会发生磁盘 I/O 的，而匿名页的 Swap 换入换出这两个操作都会发生磁盘 I/O。Linux 提供了一个 <code class="language-plaintext highlighter-rouge">/proc/sys/vm/swappiness</code> 选项，用来调整文件页和匿名页的回收倾向。swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。一般建议 swappiness 设置为 0（默认值是 60），这样在回收内存的时候，会更倾向于文件页的回收，但是并不代表不会回收匿名页。</p><ul><li>==尽早触发 kswapd 内核线程异步回收内存==</ul><p>因为后台内存回收是异步的，不会阻塞进程，所以应该尽早触发后台内存回收来避免应用进行直接内存回收。</p><h2 id="在-4gb-物理内存的机器上申请-8g-内存会怎么样">在 4GB 物理内存的机器上，申请 8G 内存会怎么样？</h2><p>在 32 位操作系统中，进程最多只能申请 3 GB 大小的虚拟内存空间，所以进程申请 8GB 内存的话，在申请虚拟内存阶段就会失败。</p><p>在 64 位操作系统中，进程可以使用 128 TB 大小的虚拟内存空间，所以进程申请 8GB 内存是没问题的，因为进程申请内存是申请虚拟内存，只要不读写这个虚拟内存，操作系统就不会分配物理内存。</p><p>如果申请物理内存大小超过了空闲物理内存大小，就要看操作系统有没有开启 Swap 机制：</p><ul><li>如果没有开启 Swap 机制，程序就会直接 OOM（被内存溢出机制杀掉）；<li>如果有开启 Swap 机制，程序可以正常运行。</ul><p>==什么是 Swap 机制？==</p><p>Swap 就是把一块磁盘空间或者本地文件，当成内存来使用，它包含换出和换入两个过程：</p><ul><li><strong>换出（Swap Out）</strong> ，是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存；<li><strong>换入（Swap In）</strong>，是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来；</ul><p>使用 Swap 机制优点是，应用程序实际可以使用的内存空间将远远超过系统的物理内存。当然，频繁地读写硬盘，会显著降低操作系统的运行速率，这也是 Swap 的弊端。</p><p>Linux 中的 Swap 机制会在内存不足和内存闲置的场景下触发：</p><ul><li><strong>内存不足</strong>：当系统需要的内存超过了可用的物理内存时，内核会将内存中不常使用的内存页交换到磁盘上为当前进程让出内存，保证正在执行的进程的可用性，这个内存回收的过程是强制的直接内存回收（Direct Page Reclaim）。直接内存回收是同步的过程，会阻塞当前申请内存的进程。<li><strong>内存闲置</strong>：应用程序在启动阶段使用的大量内存在启动后往往都不会使用，通过后台运行的守护进程（kSwapd），我们可以将这部分只使用一次的内存交换到磁盘上为其他内存的申请预留空间。kSwapd 是 Linux 负责页面置换（Page replacement）的守护进程，它也是负责交换闲置内存的主要进程，它会在空闲内存低于一定水位时，回收内存页中的空闲内存保证系统中的其他进程可以尽快获得申请的内存。kSwapd 是后台进程，所以回收内存的过程是异步的，不会阻塞当前申请内存的进程。</ul><h1 id="进程管理">进程管理</h1><h2 id="进程线程基础知识">进程、线程基础知识</h2><h3 id="进程">进程</h3><p>运行中的程序，就是进程。</p><p>==并发和并行有什么区别？==</p><p>并发就是在某个CPU核心上多个进程交替执行，它实际上仍然是串行，但是从整体上看就好像多个进程都被执行了一样，所以就是并发。而并行就是用不同的CPU核心去同时执行不同的任务。</p><p>==进程与程序的关系==</p><p>程序执行起来就是进程，CPU可以从一个进程切换到另外一个进程，但是在切换前必须要记录当前进程中运行的状态信息，方便下次切换回来的时候可以恢复执行。</p><h4 id="进程的状态">进程的状态</h4><p>完整的进程状态图：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/8-%E8%BF%9B%E7%A8%8B%E4%BA%94%E4%B8%AA%E7%8A%B6%E6%80%81.jpg" alt="进程五种状态的变迁" /></p><p>进程各个状态的意义：</p><ul><li><p><strong>创建状态</strong>（<em>new</em>）：进程正在被创建时的状态；</p><li><p><strong>就绪状态</strong>（<em>Ready</em>）：可运行，由于其他进程处于运行状态而暂时停止运行；</p><li><p><strong>运行状态</strong>（<em>Running</em>）：该时刻进程占用 CPU；</p><li><p><strong>阻塞状态</strong>（<em>Blocked</em>）：该进程正在等待某一事件发生（如等待I/O操作的完成）而暂时停止运行，此时即使给它CPU控制权，它也无法运行；</p><li><p><strong>结束状态</strong>（<em>Exit</em>）：进程正在从系统中消失时的状态；</p></ul><p>进程的状态变迁：</p><ul><li>NULL -&gt; 创建状态：一个新进程被创建时的第一个状态；<li>创建状态 -&gt; 就绪状态：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的；<li>就绪状态 -&gt; 运行状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程；<li>运行状态 -&gt; 结束状态：当进程已经运行完成或出错时，会被操作系统作结束状态处理；<li>运行状态 -&gt; 就绪状态：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行；<li>运行状态 -&gt; 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件；<li>阻塞状态 -&gt; 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态；</ul><p>此外还有<strong>挂起状态</strong>，挂起状态是由于系统和用户的需要将进程挂起，意味着该进程处于静止状态。如果进程正在执行，它将暂停执行，如果原本处于就绪状态，则该进程暂时不接受调度。比如：</p><ul><li>OS为了提高内存利用率，将暂时不能运行的程序调出磁盘<li>通过sleep让进程间歇性挂起，其工作原理是设置一个定时器，到期后唤醒进程<li>用户希望挂起一个程序的执行，比如Linux中用 <code class="language-plaintext highlighter-rouge">Ctrl+Z</code> 挂起进程，或者调试程序。</ul><h4 id="进程的控制结构">进程的控制结构</h4><p>在操作系统中，是用进程控制块（<em>process control block，PCB</em>）数据结构来描述进程的，<strong>PCB 是进程存在的唯一标识</strong>，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。</p><p>==PCB 具体包含什么信息呢？==</p><p><strong>进程描述信息：</strong></p><ul><li>进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；<li>用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；</ul><p><strong>进程控制和管理信息：</strong></p><ul><li>进程当前状态，如 new、ready、running、waiting 或 blocked 等；<li>进程优先级：进程抢占 CPU 时的优先级；</ul><p><strong>资源分配清单：</strong></p><ul><li>有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。</ul><p><strong>CPU 相关信息：</strong></p><ul><li>CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。</ul><p>==每个 PCB 是如何组织的呢？==</p><p>通常是通过<strong>链表</strong>的方式进行组织，把具有<strong>相同状态的进程链在一起，组成各种队列</strong>。比如：</p><ul><li>将所有处于就绪状态的进程链在一起，称为<strong>就绪队列</strong>；<li>把所有因等待某事件而处于等待状态的进程链在一起就组成各种<strong>阻塞队列</strong>；</ul><h4 id="进程的控制">进程的控制</h4><p>进程的创建、终止、阻塞、唤醒过程。</p><p>==创建进程==</p><p>操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源，当子进程被终止时，其在父进程处继承的资源应当还给父进程。同时，终止父进程时同时也会终止其所有的子进程。</p><p>注意：Linux 操作系统对于终止有子进程的父进程，会把子进程交给 1 号进程接管。</p><p>创建进程的过程如下：</p><ul><li>申请一个空白的 PCB，并向 PCB 中填写一些控制和管理进程的信息，比如进程的唯一标识等；<li>为该进程分配运行时所必需的资源，比如内存资源；<li>将 PCB 插入到就绪队列，等待被调度运行；</ul><p>==终止进程==</p><p>进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 <code class="language-plaintext highlighter-rouge">kill</code> 掉）。终止进程的过程如下：</p><ul><li>查找需要终止的进程的 PCB；<li>如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；<li>如果其还有子进程，则应将其所有子进程终止；<li>将该进程所拥有的全部资源都归还给父进程或操作系统；<li>将其从 PCB 所在队列中删除；</ul><p>==阻塞进程==</p><p>当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而一旦被阻塞等待，它只能由另一个进程唤醒。阻塞进程的过程如下：</p><ul><li>找到将要被阻塞进程标识号对应的 PCB；<li>如果该进程为运行状态，则保护其现场，将其状态转为阻塞状态，停止运行；<li>将该 PCB 插入到阻塞队列中去；</ul><p>==唤醒进程==</p><p>进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。唤醒进程的过程如下：</p><ul><li>在该事件的阻塞队列中找到相应进程的 PCB；<li>将其从阻塞队列中移出，并置其状态为就绪状态；<li>把该 PCB 插入到就绪队列中，等待调度程序调度；</ul><p>进程的阻塞和唤醒是一对功能相反的语句，如果某个进程调用了阻塞语句，则必有一个与之对应的唤醒语句。</p><h4 id="进程的上下文切换">进程的上下文切换</h4><p>各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，一个进程切换到另一个进程运行，称为进程的上下文切换。</p><p>==CPU 上下文切换==</p><p>CPU在运行每个任务之前，它需要知道任务是从哪里加载，又从哪里开始运行的。CPU寄存器保存了相关数据，程序计数器则是用来存储CPU 正在执行的指令位置、或者即将执行的下一条指令位置。所以说，CPU 寄存器和程序计数是 CPU 在运行任何任务前，所必须依赖的环境，这些环境就叫做 <strong>CPU 上下文</strong>。</p><p>CPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。</p><p>任务主要包含进程、线程和中断。所以，根据任务的不同，CPU 上下文切换可以分成：<strong>进程上下文切换、线程上下文切换和中断上下文切换</strong>。</p><p>==进程的上下文切换到底是切换什么呢？==</p><p>进程是由内核管理和调度的，所以进程的切换只能发生在内核态。所以，<strong>进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。</strong>通常，会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文，然后恢复到 CPU 中，这使得这个进程可以继续执行。</p><p>==发生进程上下文切换有哪些场景？==</p><ul><li>时间轮转片用完。为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；<li>进程在系统资源不足时，等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；<li>当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；<li>发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；<li>当进程将自己主动挂起时，自然也会重新调度。比如sleep睡眠函数，ctrl+z命令挂起。</ul><h3 id="线程">线程</h3><p>==什么是线程？==</p><p>线程是进程当中的一条执行流程。同一个进程内多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，这样可以确保线程的控制流是相对独立的。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/16-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.jpg" alt="多线程" /></p><p>==线程的优缺点？==</p><p>线程的优点：</p><ul><li>一个进程中可以同时存在多个线程；<li>各个线程之间可以并发执行；<li>各个线程之间可以共享地址空间和文件等资源；</ul><p>线程的缺点：</p><ul><li>当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃</ul><h4 id="线程与进程的比较">线程与进程的比较</h4><ul><li>进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位；<li>进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；<li>线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；<li>线程能减少并发执行的时间和空间开销；</ul><p>对于，线程相比进程能减少开销，体现在：</p><ul><li>线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；<li>线程的终止时间比进程快，因为线程释放的资源相比进程少很多；<li>同一个进程内的线程切换比进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的；<li>由于同一进程的各线程间共享内存和文件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；</ul><p>所以，不管是时间效率，还是空间效率线程比进程都要高。</p><h4 id="线程的上下文切换">线程的上下文切换</h4><p>线程与进程最大的区别在于：<strong>线程是调度的基本单位，而进程则是资源拥有的基本单位</strong>。</p><p>所以，所谓操作系统的任务调度，实际上的调度对象是线程，进程只是给线程提供了资源。</p><p>线程的上下文切换需要看是否属于同一个进程：</p><ul><li>当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；<li><strong>当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据</strong>；</ul><h4 id="线程的实现">线程的实现</h4><p>主要有三种线程的实现方式：</p><ul><li><strong>用户线程（User Thread）</strong>：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；<li><strong>内核线程（Kernel Thread）</strong>：在内核中实现的线程，是由内核管理的线程；<li><strong>轻量级进程（LightWeight Process）</strong>：在内核中来支持用户线程；</ul><p>用户线程和内核线程有多对一、一对一、一对多的对应关系。</p><p>==用户线程如何理解？存在什么优势和缺陷？==</p><p>用户线程是基于用户态的线程管理库来实现的，那么<strong>线程控制块（*Thread Control Block, TCB*）</strong> 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。所以，<strong>用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。</strong>用户级线程的模型，是多对一的关系，多个用户线程对应同一个内核线程。</p><p>用户线程的<strong>优点</strong>：</p><ul><li>每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；<li>用户线程的切换也是由线程库函数来完成的，无需用户态与内核态的切换，所以速度特别快；</ul><p>用户线程的<strong>缺点</strong>：</p><ul><li>由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。<li>当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。<li>由于时间片分配给进程，故与其他进程比，在多线程执行时，每个线程得到的时间片较少，执行会比较慢；</ul><p>==那内核线程如何理解？存在什么优势和缺陷？==</p><p><strong>内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。</strong>内核线程的模型是一对一的关系，一个用户线程对应一个内核线程。</p><p>内核线程的<strong>优点</strong>：</p><ul><li>在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；<li>分配给线程，多线程的进程获得更多的 CPU 运行时间；</ul><p>内核线程的<strong>缺点</strong>：</p><ul><li>在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB；<li>线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；</ul><p>==轻量级进程如何理解？==</p><p>轻量级进程（Light-weight process，LWP）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持。</p><h3 id="调度">调度</h3><h4 id="调度时机">调度时机</h4><p>在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，就会触发一次调度。</p><ul><li><em>从就绪态 -&gt; 运行态</em>：当进程被创建时，会进入到就绪队列，操作系统会从就绪队列选择一个进程运行；<li><em>从运行态 -&gt; 阻塞态</em>：当进程发生 I/O 事件而阻塞时，操作系统必须选择另外一个进程运行；<li><em>从运行态 -&gt; 结束态</em>：当进程退出结束后，操作系统得从就绪队列选择另外一个进程运行；</ul><h4 id="调度原则">调度原则</h4><ul><li><strong>CPU 利用率</strong>：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；<li><strong>系统吞吐量</strong>：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；<li><strong>周转时间</strong>：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；<li><strong>等待时间</strong>：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；<li><strong>响应时间</strong>：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。</ul><h4 id="调度算法">调度算法</h4><p><strong>单核 CPU 系统</strong>中常见的调度算法：</p><p>==先来先服务调度算法==</p><p>最简单的一个调度算法，就是非抢占式的先来先服务（First Come First Serve, FCFS）算法，每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择下一个进程运行。</p><p>特点：对长作业有理，适用于CPU繁忙型作业的系统，不利于短作业，不适用月I/O繁忙型作业的系统。</p><p>==最短作业优先调度算法==</p><p>最短作业优先（Shortest Job First, SJF）调度算法会优先选择运行时间最短的进程来运行。</p><p>特点：有助于提高系统的吞吐量，但是对长作业不利。</p><p>==高响应比优先调度算法==</p><p><strong>高响应比优先 （Highest Response Ratio Next, HRRN）调度算法</strong>主要是权衡了短作业和长作业。每次进行调度时，先计算「响应比优先级」，然后调用「响应比优先级」最高的进程，「响应比优先级」的计算公式：</p><p><code class="language-plaintext highlighter-rouge">优先权=（等待时间+要求服务的时间）/要求服务的时间</code>，从公式可以发现：</p><ul><li>如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；<li>如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；</ul><p>==时间片轮转调度算法==</p><p>最简单、最公平且使用最广的算法就是<strong>时间片轮转（Round Robin, RR）调度算法</strong>。每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。</p><ul><li>如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；<li>如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；</ul><p>另外，时间片的长度就是一个很关键的点：</p><ul><li>如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；<li>如果设得太长又可能引起对短作业进程的响应时间变长。将</ul><p>==最高优先级调度算法==</p><p>从就绪队列中选择最高优先级的进程进行运行，这称为<strong>最高优先级（Highest Priority First，HPF）调度算法。</strong>进程的优先级可以分为，静态优先级和动态优先级：</p><ul><li>静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；<li>动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级</ul><p>该算法针对优先级高的进程有两种处理方法，非抢占式和抢占式：</p><ul><li>非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。<li>抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。</ul><p>缺点就是：可能导致优先级低的进程永远不会运行。</p><p>==多级反馈队列调度算法==</p><p><strong>多级反馈队列（Multilevel Feedback Queue）调度算法</strong>是「时间片轮转算法」和「最高优先级算法」的综合和发展。具体为：</p><ul><li>「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。<li>「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；</ul><p>可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也变更长了，所以该算法很好的<strong>兼顾了长短作业，同时有较好的响应时间。</strong></p><h2 id="进程间有哪些通信方式">进程间有哪些通信方式</h2><p>每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。</p><h3 id="管道">管道</h3><p>管道是两个进程间的一条通道，一端负责投递，另一端负责输出。管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行。</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span text-data=" Shell "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="nv">$ </span>ps auxf | <span class="nb">grep </span>mysql <span class="c"># 查看是否有mysql相关的进程在运行</span>
</pre></table></code></div></div><p>上面命令行里的「<code class="language-plaintext highlighter-rouge">|</code>」竖线就是一个<strong>管道</strong>，它的功能是将前一个命令（<code class="language-plaintext highlighter-rouge">ps auxf</code>）的输出，作为后一个命令（<code class="language-plaintext highlighter-rouge">grep mysql</code>）的输入。通过管道这种方式，ps和grep这两个命令对应的进程进行了一次合作。</p><p>管道通常有两类：</p><ul><li>匿名管道。通过<code class="language-plaintext highlighter-rouge">pipe</code>的系统调用创建，即「<code class="language-plaintext highlighter-rouge">|</code>」，没有名字，用完了就销毁。<li>命名管道。使用<code class="language-plaintext highlighter-rouge">mkfifo</code>命令创建的，并且指定管道名字。</ul><p>管道的特点就是简单，但是这种通信方式效率低，不适合进程间频繁地交换数据。</p><p>==管道如何创建的，背后原理是什么？==</p><p>匿名管道的创建，需要调用pipe系统调用，这个调用返回两个文件描述符，一个是管道的读取端描述符 <code class="language-plaintext highlighter-rouge">fd[0]</code>，另一个是管道的写入端描述符 <code class="language-plaintext highlighter-rouge">fd[1]</code>。需要注意，匿名管道是特殊的文件，只存在于内存，不存在于文件系统中。但是，这两个描述符都在一个进程里面，是怎么起到进程间通信的？</p><p>我们可以使用<code class="language-plaintext highlighter-rouge">fork</code>创建子进程，<strong>创建的子进程会复制父进程的文件描述符</strong>，这样就做到了两个进程各有两个「 <code class="language-plaintext highlighter-rouge">fd[0]</code> 与 <code class="language-plaintext highlighter-rouge">fd[1]</code>」，两个进程就可以通过各自的 fd 写入和读取同一个管道文件实现跨进程通信了。管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：</p><ul><li>父进程关闭读取的 fd[0]，只保留写入的 fd[1]；<li>子进程关闭写入的 fd[1]，只保留读取的 fd[0]；</ul><p>所以说如果需要双向通信，则应该创建两个管道。</p><div class="table-wrapper"><table><tbody><tr><td>需要注意的是，在shell里面执行A<td>B命令的时候，两个进程都是shell创建出来的子进程。所以说，在 shell 里通过「<code class="language-plaintext highlighter-rouge">|</code>」匿名管道将多个命令连接在一起，实际上也就是创建了多个子进程。</table></div><p>总结一下：</p><p><strong>对于匿名管道，它的通信范围是存在父子关系的进程</strong>。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。</p><p><strong>对于命名管道，它可以在不相关的进程间也能相互通信</strong>。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。</p><p>不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循<strong>先进先出</strong>原则。</p><h3 id="消息队列">消息队列</h3><p>消息队列比较灵活，它支持同时存在多个发送者和多个接收者，可以用于进程间频繁的交换数据。</p><p>消息队列是链表设计实现的队列。当创建新的消息队列时，内核从系统内存分配一个队列数据结构，其中有消息头部指针和权限，队列的消息由头部指针引出，每个消息都会指向下一个消息的指针。在消息的结构体中，除了指向下一个消息的指针之外，就是消息的内容。消息内容包含类型和数据两部分，数据是一段内存数据，类型是用户程序为每个消息指定的。内核不需要知道具体是什么类型，仅仅只是保存，以及基于类型进行简单的查找。</p><p>消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。</p><p>消息队列的缺点：</p><ul><li>通信不及时，存在队列中，可能出现消息延迟<li>不适合比较大数据的传输，因为在内核中每个消息体都有一个最大长度的限制，同时所有队列所包含的全部消息体的总长度也是有上限。<li>消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。</ul><h3 id="共享内存">共享内存</h3><p>消息队列的读取和写入的过程，需要发生用户态和内核态之间的消息拷贝过程，共享内存不需要。</p><p><strong>共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中</strong>。这样这个进程写入的东西，另外一个进程马上就能看到了，不需要拷贝来拷贝去，大大提高了进程间通信的速度。</p><h3 id="信号量">信号量</h3><p>共享内存会有一个问题，就是多个进程同时修改同一个内存，可能会出现数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问，信号量就实现了这一保护机制。</p><p>信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步。信号量表示资源的数量，控制信号量的方式有两种原子操作：</p><ul><li>一个是 <strong>P 操作</strong>，这个操作会把信号量减去 1，相减后如果信号量 &lt; 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 &gt;= 0，则表明还有资源可使用，进程可正常继续执行。<li>另一个是 <strong>V 操作</strong>，这个操作会把信号量加上 1，相加后如果信号量 &lt;= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 &gt; 0，则表明当前没有阻塞中的进程；</ul><p>P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/10-%E4%BF%A1%E5%8F%B7%E9%87%8F-%E4%BA%92%E6%96%A5.jpg" alt="img" /></p><ul><li>进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。<li>若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。<li>直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。</ul><p>可以发现，信号初始化为 <code class="language-plaintext highlighter-rouge">1</code>，就代表着是<strong>互斥信号量</strong>，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。</p><p>在多个进程里，每个进程并不一定是顺序执行的，如果我们希望多个进程能够合作，比如一个进程负责生产数据，另一个进程负责读取数据。这个时候可以通过信号量来实现多进程同步，初始化信号量为0。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/11-%E4%BF%A1%E5%8F%B7%E9%87%8F-%E5%90%8C%E6%AD%A5.jpg" alt="img" /></p><ul><li>如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待；<li>接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B；<li>最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。</ul><p>可以发现，信号初始化为 <code class="language-plaintext highlighter-rouge">0</code>，就代表着是<strong>同步信号量</strong>，它可以保证进程 A 应在进程 B 之前执行。</p><h3 id="信号">信号</h3><p>上面说的进程间通信，都是常规状态下的工作模式。<strong>对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。</strong></p><p>信号是进程间通信机制中<strong>唯一的异步通信机制</strong>，因为可以在任何时候发送信号给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。</p><ul><li><p><strong>执行默认操作</strong>。Linux 对每种信号都规定了默认操作，收到信号就执行其对应的操作。</p><li><p><strong>捕捉信号</strong>。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。</p><li><p><strong>3.忽略信号</strong>。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 <code class="language-plaintext highlighter-rouge">SIGKILL</code> 和 <code class="language-plaintext highlighter-rouge">SEGSTOP</code>，它们用于在任何时候中断或结束某一进程</p></ul><p>信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）：</p><ul><li>Ctrl+C 产生 <code class="language-plaintext highlighter-rouge">SIGINT</code> 信号，表示终止该进程；<li>Ctrl+Z 产生 <code class="language-plaintext highlighter-rouge">SIGTSTP</code> 信号，表示停止该进程，但还未结束；<li>kill -9 1050 ，表示给 PID 为 1050 的进程发送 <code class="language-plaintext highlighter-rouge">SIGKILL</code> 信号，用来立即结束该进程；</ul><h3 id="socket">Socket</h3><p>前面提到的管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想<strong>跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。</strong>当然，也可以在通主机上进程间通信。</p><p>创建 socket 的系统调用：<code class="language-plaintext highlighter-rouge">int socket(int domain, int type, int protocal)</code></p><ul><li>domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；<li>type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP；SOCK_DGRAM 表示的是数据报，对应 UDP；SOCK_RAW 表示的是原始套接字；<li>protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；</ul><p>根据创建 socket 类型的不同，通信的方式也就不同：</p><ul><li>实现 TCP 字节流通信： socket 类型是 AF_INET 和 SOCK_STREAM；<li>实现 UDP 数据报通信：socket 类型是 AF_INET 和 SOCK_DGRAM；<li>实现本地进程间通信： 「本地字节流 socket 」类型是 AF_LOCAL 和 SOCK_STREAM，「本地数据报 socket 」类型是 AF_LOCAL 和 SOCK_DGRAM。另外，AF_UNIX 和 AF_LOCAL 是等价的。</ul><p>==针对 TCP 协议通信的 socket 编程模型==</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/12-TCP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg" alt="img" /></p><ul><li>服务端和客户端初始化 <code class="language-plaintext highlighter-rouge">socket</code>，得到文件描述符；<li>服务端调用 <code class="language-plaintext highlighter-rouge">bind</code>，将绑定在 IP 地址和端口;<li>服务端调用 <code class="language-plaintext highlighter-rouge">listen</code>，进行监听；<li>服务端调用 <code class="language-plaintext highlighter-rouge">accept</code>，等待客户端连接；<li>客户端调用 <code class="language-plaintext highlighter-rouge">connect</code>，向服务器端的地址和端口发起连接请求；<li>服务端 <code class="language-plaintext highlighter-rouge">accept</code> 返回用于传输的 <code class="language-plaintext highlighter-rouge">socket</code> 的文件描述符；<li>客户端调用 <code class="language-plaintext highlighter-rouge">write</code> 写入数据；服务端调用 <code class="language-plaintext highlighter-rouge">read</code> 读取数据；<li>客户端断开连接时，会调用 <code class="language-plaintext highlighter-rouge">close</code>，那么服务端 <code class="language-plaintext highlighter-rouge">read</code> 读取数据的时候，就会读取到了 <code class="language-plaintext highlighter-rouge">EOF</code>，待处理完数据后，服务端调用 <code class="language-plaintext highlighter-rouge">close</code>，表示连接关闭。</ul><p>这里需要注意的是，服务端调用 <code class="language-plaintext highlighter-rouge">accept</code> 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。所以，监听的 socket 和真正用来传送数据的 socket，是「<strong>两个</strong>」 socket，一个叫作<strong>监听 socket</strong>，一个叫作<strong>已完成连接 socket</strong>。</p><p>==针对 UDP 协议通信的 socket 编程模型==</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/13-UDP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg" alt="img" /></p><p>UDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。</p><p>另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。</p><p>==针对本地进程间通信的 socket 编程模型==</p><p>本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是<strong>绑定一个本地文件</strong>，这也就是它们之间的最大区别。</p><h2 id="多线程冲入了怎么办">多线程冲入了怎么办？</h2><h3 id="互斥的概念">互斥的概念</h3><p>当多线程相互竞争操作共享变量时，如果在执行过程中发生了上下文切换会得到错误的结果，事实上，每次运行都可能得到不同的结果，因此输出的结果存在不确定性。</p><p>由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为<strong>临界区（critical section），它是访问共享资源的代码片段，一定不能给多线程同时执行。</strong></p><p>互斥：保证一个线程在临界区执行时，其它线程应该被阻止进入临界区。</p><h3 id="同步的概念">同步的概念</h3><p>同步，就是并发进程/线程在一些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。</p><p>同步与互斥是两种不同的概念：</p><ul><li>同步就好比：「操作 A 应在操作 B 之前执行」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等；<li>互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」；</ul><p>为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施，主要的方法有两种：</p><ul><li><em>锁</em>：加锁、解锁操作；<li><em>信号量</em>：P、V 操作；</ul><p>这两个都可以方便地实现进程/线程互斥，信号量比锁的功能更强，它还可以实现进程/线程同步。</p><h3 id="锁">锁</h3><p>使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。根据锁的实现不同，可以分为「忙等待锁」和「无忙等待锁」：</p><p>==忙等待锁」的实现==</p><p>当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为自旋锁。这是最简单的一种锁，一直自旋，利用 CPU 周期，直到锁可用。在单处理器上，需要抢占式的调度器。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。</p><p>==「无等待锁」的实现==</p><p>无等待锁顾明思议就是获取不到锁的时候，不用自旋，既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。</p><h3 id="信号量-1">信号量</h3><p>信号量不仅可以实现临界区的互斥访问控制，还可以线程间的事件同步。</p><p>信号量初始值设置为1，可以实现互斥访问，互斥信号量的值仅取 1、0 和 -1 三个值，分别表示：</p><ul><li>如果互斥信号量为 1，表示没有线程进入临界区；<li>如果互斥信号量为 0，表示有一个线程进入临界区；<li>如果互斥信号量为 -1，表示一个线程进入临界区，另一个线程等待进入。</ul><p>信号量初始值设置为0，可以实现事件同步。</p><h3 id="生产者-消费者问题">生产者-消费者问题</h3><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/20-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85.jpg" alt="生产者-消费者模型" /></p><p>生产者-消费者问题描述：</p><ul><li><strong>生产者</strong>在生成数据后，放在一个缓冲区中；<li><strong>消费者</strong>从缓冲区取出数据处理；<li>任何时刻，<strong>只能有一个</strong>生产者或消费者可以访问缓冲区；</ul><p>我们对问题分析可以得出：</p><ul><li>任何时刻只能有一个线程操作缓冲区，说明操作缓冲区是临界代码，<strong>需要互斥</strong>；<li>缓冲区空时，消费者必须等待生产者生成数据；缓冲区满时，生产者必须等待消费者取出数据。说明生产者和消费者<strong>需要同步</strong>。</ul><p>那么我们需要三个信号量，分别是：</p><ul><li>互斥信号量 <code class="language-plaintext highlighter-rouge">mutex</code>：用于互斥访问缓冲区，初始化值为 1；<li>资源信号量 <code class="language-plaintext highlighter-rouge">fullBuffers</code>：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；<li>资源信号量 <code class="language-plaintext highlighter-rouge">emptyBuffers</code>：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；</ul><h3 id="哲学家就餐问题">哲学家就餐问题</h3><p>先来看看哲学家就餐的问题描述：</p><ul><li><code class="language-plaintext highlighter-rouge">5</code> 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；<li>巧就巧在，这个桌子只有 <code class="language-plaintext highlighter-rouge">5</code> 支叉子，每两个哲学家之间放一支叉子；<li>哲学家围在一起先思考，思考中途饿了就会想进餐；<li><strong>奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐</strong>；<li><strong>吃完后，会把两支叉子放回原处，继续思考</strong>；</ul><p>那么问题来了，如何保证哲 学家们的动作有序进行，而不会出现有人永远拿不到叉子呢？</p><p>==方案一==</p><p>用信号量的方式，拿起叉子用P操作，代表有叉子直接用，没有叉子时就等待其它哲学家。不过，这种解法存在一个极端的问题：假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会等待右边的叉子，很明显这发生了死锁的现象。</p><p>==方案二==</p><p>我们在拿叉子前，加入互斥信号，只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。方案二虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。</p><p>==方案三==</p><p>让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。既不会出现死锁，也可以两人同时进餐。</p><h2 id="怎么避免死锁">怎么避免死锁？</h2><h3 id="死锁的概念">死锁的概念</h3><p>当两个线程为了保护两个不同的共享资源而使用了两个互斥锁，如果这两个互斥锁应用不当，可能会造成<strong>两个线程都在等待对方释放锁的情况</strong>，在没有外力的作用下，线程会一直相互等待，就没办法继续运行，这种情况就是发生了<strong>死锁</strong>。</p><p>死锁只有同时满足以下四个条件才会发生：</p><ul><li>互斥条件；<li>持有并等待条件；<li>不可剥夺条件；<li>环路等待条件；</ul><p>==互斥条件==</p><p>互斥条件是指多个线程不能同时使用同一个资源。</p><p>比如线程 A 已经持有的资源，不能再同时被线程 B 持有，如果线程 B 请求获取线程 A 已经占用的资源，那线程 B 只能等待，直到线程 A 释放了资源。</p><p>==持有并等待条件==</p><p>简单点说，就是线程持有一个资源并且在等待另一个资源，比如当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程 A 就会处于等待状态，但是<strong>线程 A 在等待资源 2 的同时并不会释放自己已经持有的资源 1</strong>。</p><p>==不可剥夺条件==</p><p>不可剥夺条件是指，当线程已经持有了资源 ，<strong>在自己使用完之前不能被其他线程获取</strong>，比如线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。</p><p>==环路等待条件==</p><p>环路等待条件指的是，在死锁发生的时候，<strong>两个线程获取资源的顺序构成了环形链</strong>。比如线程 A 已经持有资源 2，而想请求资源 1， 线程 B 已经持有资源 1，而想请求资源 2，这就形成资源请求等待的环形图。</p><h3 id="代码实现">代码实现</h3><div class="language-c highlighter-rouge"><div class="code-header"> <span text-data=" C "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c1">// 线程A B分别运行代码片段 并且两个线程共享两把全局的互斥锁A和B</span>
<span class="c1">// 线程A 函数</span>
<span class="kt">void</span> <span class="nf">proc_A</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">lock</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>  <span class="c1">// 获取锁A</span>
    <span class="n">lock</span><span class="p">(</span><span class="n">B</span><span class="p">);</span>  <span class="c1">// 获取锁B</span>
    <span class="n">unlock</span><span class="p">(</span><span class="n">B</span><span class="p">);</span>
    <span class="n">unlock</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
<span class="p">}</span>
<span class="c1">// 线程B 函数</span>
<span class="kt">void</span> <span class="nf">proc_B</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">lock</span><span class="p">(</span><span class="n">B</span><span class="p">);</span>
    <span class="n">lock</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
    <span class="n">unlock</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>
    <span class="n">unlock</span><span class="p">(</span><span class="n">B</span><span class="p">);</span>
<span class="p">}</span>
</pre></table></code></div></div><h3 id="避免死锁问题的发生">避免死锁问题的发生</h3><p>那么避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是<strong>使用资源有序分配法，来破环环路等待条件</strong>。</p><p><strong>那什么是资源有序分配法呢？</strong>线程 A 和 线程 B 获取资源的顺序要一样，当线程 A 是先尝试获取资源 A，然后尝试获取资源 B 的时候，线程 B 同样也是先尝试获取资源 A，然后尝试获取资源 B。也就是说，线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。</p><h2 id="什么是悲观锁乐观锁">什么是悲观锁、乐观锁？</h2><h3 id="互斥锁与自旋锁">互斥锁与自旋锁</h3><p>当已经有一个线程加锁后，其他线程加锁会失败，互斥锁和自旋锁对加锁失败后的处理方式不一样：</p><ul><li><strong>互斥锁</strong>加锁失败后，线程会<strong>释放 CPU</strong> ，给其他线程；<li><strong>自旋锁</strong>加锁失败后，线程会<strong>忙等待</strong>，直到它拿到锁；</ul><p><strong>互斥锁</strong>是一种「独占锁」，比如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放手中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，既然线程 B 释放掉了 CPU，自然线程 B 加锁的代码就会被阻塞。对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的。当加锁失败时，内核会将线程从运行状态置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。因为线程运行状态改变，需要内核切换线程，所以会有两次线程上下文切花你的成本。</p><p><strong>自旋锁</strong>是通过 CPU 提供的 <code class="language-plaintext highlighter-rouge">CAS</code> 函数（<em>Compare And Swap</em>），在「用户态」完成加锁和解锁操作，不会主动产生线程上下文切换，所以相比互斥锁来说，会快一些，开销也小一些。一般加锁的过程：</p><ul><li>第一步，查看锁的状态，如果锁是空闲的，则执行第二步；<li>第二步，将锁设置为当前线程持有；</ul><p>使用自旋锁的时候，当发生多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这里的「忙等待」可以用 <code class="language-plaintext highlighter-rouge">while</code> 循环等待实现。需要注意，在单核 CPU 上，需要抢占式的调度器。否则，自旋锁在单 CPU 上无法使用，因为一个自旋的线程永远不会放弃 CPU。</p><p>它俩是<strong>锁的最基本处理方式</strong>，更高级的锁都会选择其中一个来实现，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。</p><h3 id="读写锁">读写锁</h3><p>读写锁适用于能明确区分读操作和写操作的场景。读写锁的工作原理是：</p><ul><li>当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。<li>但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。</ul><p>所以说，写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。</p><p><strong>读优先锁</strong>对于读线程并发性更好，但也不是没有问题。我们试想一下，如果一直有读线程获取读锁，那么写线程将永远获取不到写锁，这就造成了写线程「饥饿」的现象。</p><p><strong>写优先锁</strong>可以保证写线程不会饿死，但是如果一直有写线程获取写锁，读线程也会被「饿死」。</p><p><strong>公平读写锁</strong>比较简单的一种方式是：用队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。</p><h3 id="乐观锁与悲观锁">乐观锁与悲观锁</h3><p>悲观锁做事比较悲观，它认为<strong>多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁</strong>。前面提到的互斥锁、自旋锁、读写锁，都是属于悲观锁。</p><p>乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：<strong>先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作</strong>。</p><p>乐观锁放弃后重试的成本可能很高，但是冲突的概率比较低的话，还是可以接受的。</p><p>比如在线编辑就使用了乐观锁：允许多人同时编辑，而不是一个用户在编辑文档，另一个用户就无法打开相同的文档。具体原理是：由于发生冲突的概率比较低，所以先让用户编辑文档，但是浏览器在下载文档时会记录下服务端返回的文档版本号；当用户提交修改时，发给服务端的请求会带上原始文档版本号，服务器收到后将它与当前版本号进行比较，如果版本号一致则修改成功，否则提交失败。</p><p>此外，Git也用了乐观锁的思想，先让用户编辑代码，然后提交的会后，通过版本号来判断是否产生了冲突，发生冲突的地方，需要我们修改后再重新提交。</p><p><strong>乐观锁、悲观锁使用场景</strong>：乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。</p><h2 id="一个进程最多可以创建多少个线程">一个进程最多可以创建多少个线程？</h2><p>这个问题跟两个东西有关系：</p><ul><li><strong>进程的虚拟内存空间上限</strong>，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。<li><strong>系统参数限制</strong>，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数。</ul><p>那么在进程里创建一个线程需要消耗多少虚拟内存大小？</p><p>我们可以执行 ulimit -a 这条命令，查看进程创建线程时默认分配的栈空间大小，假如默认分配给线程的栈空间大小为 8M。在32位系统中有3G虚拟内存可以使用，差不多可以创建300个线程。如果想要创建更多的线程，可以调整创建线程时分配的栈空间大小。</p><p>如果是64位系统，用户空间的虚拟内存最大值是128T，算起来可以创建很多进程，但是除了虚拟内存的限制，还会受系统的参数或者性能限制：</p><ul><li><strong><em>proc/sys/kernel/threads-max</em></strong>，表示系统支持的最大线程数，默认值是 <code class="language-plaintext highlighter-rouge">14553</code>；<li><strong><em>/proc/sys/kernel/pid_max</em></strong>，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 <code class="language-plaintext highlighter-rouge">32768</code>；<li><strong><em>/proc/sys/vm/max_map_count</em></strong>，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 <code class="language-plaintext highlighter-rouge">65530</code>。</ul><p>当然这些参数可以手动调整，如果一直调大的话，具体创建多少个就要受到CPU核心的性能影响了。</p><h2 id="线程崩溃了进程也会崩溃吗">线程崩溃了，进程也会崩溃吗？</h2><h3 id="线程崩溃进程一定会崩溃吗">线程崩溃，进程一定会崩溃吗</h3><p>一般来说如果线程是因为非法访问内存引起的崩溃，那么进程肯定会崩溃，为什么系统要让进程崩溃呢，这主要是因为在进程中，<strong>各个线程的地址空间是共享的</strong>，既然是共享，那么某个线程对地址的非法访问就会导致内存的不确定性，进而可能会影响到其他线程，这种操作是危险的，操作系统会认为这很可能导致一系列严重的后果，于是干脆让整个进程崩溃。线程共享代码段，数据段，地址空间，文件非法访问内存常见的有以下几种情况：</p><ul><li><p>针对只读内存写入数据，崩溃</p><li>访问了进程没有权限访问的地址空间，崩溃<li>访问了不存在的内存，崩溃</ul><p>以上错误都是访问内存时的错误，统一会报 Segment Fault 错误（即段错误），这些都会导致进程崩溃。</p><h3 id="进程是如何崩溃的-信号机制简介">进程是如何崩溃的-信号机制简介</h3><p>通过给进程发送信号来终止进程的执行，发个信号进程怎么就崩溃了呢，这背后的原理到底是怎样的？</p><ol><li>CPU 执行正常的进程指令<li>调用 kill 系统调用向进程发送信号<li>进程收到操作系统发的信号，CPU 暂停当前程序运行，并将控制权转交给操作系统<li>调用 kill 系统调用向进程发送信号（假设为 11，即 SIGSEGV，一般非法访问内存报的都是这个错误）<li>操作系统根据情况执行相应的信号处理程序（函数），一般执行完信号处理程序逻辑后会让进程退出</ol><p>如果进程没有注册自己的信号处理函数，操作系统会执行默认的信号处理程序，如果注册了，则会执行自己的信号处理函数，可以在退出进程前执行相关的逻辑或者直接忽略信号。kill -9 命令例外，不管进程是否定义了信号处理函数，都会马上被干掉。</p><h3 id="为什么线程崩溃不会导致-jvm-进程崩溃">为什么线程崩溃不会导致 JVM 进程崩溃</h3><p>因为 JVM 自定义了自己的信号处理函数，拦截了进程终止信号，针对这两者不让它们崩溃。</p><h1 id="调度算法-1">调度算法</h1><h2 id="进程调度算法">进程调度算法</h2><h2 id="内存页面置换算法">内存页面置换算法</h2><p>当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断（出发缺页异常），请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：</p><ul><li>缺页中断在指令执行「期间」产生和处理中断信号，而一般中断在一条指令执行「完成」后检查和处理中断信号。<li>缺页中断返回该指令的开始重新执行「该指令」，而一般中断返回到该指令的「下一个指令」执行。</ul><p>缺页中断的处理流程：</p><ol><li>如果。程序需要访问内存，CPU会去找对应的页表项。<li>如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。<li>操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置。<li>找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。<li>页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。<li>最后，CPU 重新执行导致缺页异常的指令。</ol><p>如果能够在物理内存中找到空闲页，就是上面的流程，如果找不到，说明内存已经满了，这时候就需要<strong>页面置换算法</strong>选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。</p><p>页表项通常包括以下字段：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E9%A1%B5%E8%A1%A8%E9%A1%B9%E5%AD%97%E6%AE%B5.png" alt="img" /></p><ul><li><em>状态位</em>：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。<li><em>访问字段</em>：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。<li><em>修改位</em>：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。<li><em>硬盘地址</em>：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。</ul><p><strong>当出现缺页异常，需调入新页面而内存已满时，就需要页面置换算法选择一个物理页面换出到磁盘，然后把需要访问的页面换入到物理页。</strong>那其算法目标则是，尽可能减少页面的换入换出的次数，常见的页面置换算法有如下几种：</p><ul><li>最佳页面置换算法（<em>OPT</em>）<li>先进先出置换算法（<em>FIFO</em>）<li>最近最久未使用的置换算法（<em>LRU</em>）<li>时钟页面置换算法（<em>Lock</em>）<li>最不常用置换算法（<em>LFU</em>）</ul><p>==最佳页面置换算法==</p><p>最佳页面置换算法基本思路是，<strong>置换「未来」最长时间不访问的页面</strong>。所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。但是这再实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。所以该算法的作用是作为一种标准，来衡量其它页面置换算法的效率。</p><p>==先进先出置换算法==</p><p><strong>选择在内存驻留时间很长的页面进行中置换</strong>，这个就是「先进先出置换」算法的思想。性能较差。</p><p>==最近最久未使用的置换算法==</p><p>最近最久未使用（<em>LRU</em>）的置换算法的基本思路是，发生缺页时，<strong>选择最长时间没有被访问的页面进行置换</strong>，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。</p><p>虽然 LRU 在理论上是可以实现的，但是开销太大。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。</p><p>==时钟页面置换算法==</p><p>时钟页面置换算法跟 LRU 近似，又是对 FIFO 的一种改进。所以能够兼顾优化置换的次数和方便实现。</p><p>该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。当发生缺页中断时，算法首先检查表针指向的页面：</p><ul><li>如果它的访问位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；<li>如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；</ul><p>==最不常用算法==</p><p>最不常用（<em>LFU</em>）算法当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰。</p><p>它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。</p><p>增加计数器的硬件成本比较高，而且如果链表长度很大，查找访问最少的页面效率不高。</p><h2 id="磁盘调度算法">磁盘调度算法</h2><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E7%BB%93%E6%9E%84.jpg" alt="磁盘的结构" /></p><p>常见的机械磁盘是上图左边的样子，中间圆的部分是磁盘的盘片，一般会有多个盘片，每个盘面都有自己的磁头。右边的图就是一个盘片的结构，盘片中的每一层分为多个磁道，每个磁道分多个扇区，每个扇区是 <code class="language-plaintext highlighter-rouge">512</code> 字节。那么，多个具有相同编号的磁道形成一个圆柱，称之为磁盘的柱面，如上图里中间的样子。</p><p>磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘访问请求顺序来做到。</p><p>常见的磁盘调度算法有：</p><ul><li>先来先服务算法<li>最短寻道时间优先算法<li>扫描算法算法<li>循环扫描算法<li>LOOK 与 C-LOOK 算法</ul><p>==先来先服务==</p><p>先到的先服务（FCFS），比较简单，但是如果磁道很分散，性能就会很差。</p><p>==最短寻道时间优先==</p><p>最短寻道时间优先（Shortest Seek First，SSF）是优先选择从当前磁头位置所需寻道时间最短的请求，</p><p>比FCFS性能好，但是可能存在某些请求的饥饿，产生饥饿的原因是磁头只在某一小块区域来回移动。</p><p>==扫描算法==</p><p>为了防止请求饥饿的问题，可以让磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（Scan）算法。</p><p>扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题：中间部分的磁道会比较占便宜，中间部分相比其他部分响应的频率会比较多，也就是说每个磁道的响应频率存在差异。</p><p>==循环扫描算法==</p><p>扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。<strong>循环扫描（Circular Scan, CSCAN）</strong>规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应一个方向上的请求。</p><p>循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。</p><p>==LOOK 与 C-LOOK算法==</p><p>我们前面说到的扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。这其实是可以优化的，优化的思路就是<strong>磁头在移动到「最远的请求」位置，然后立即反向移动。</strong></p><p>针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，<strong>反向移动的途中会响应请求</strong>。</p><p>针对 C-SCAN 算法的优化则叫 C-LOOK，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，<strong>反向移动的途中不会响应请求</strong>。</p><h1 id="文件系统">文件系统</h1><h2 id="文件系统全家桶">文件系统全家桶</h2><h3 id="文件系统的基本组成">文件系统的基本组成</h3><p>在Linux上，<strong>一切皆文件</strong>，不仅普通的文件和目录，就连设备、管道、socket等也都是交给文件系统管理。</p><p>Linux 文件系统会为每个文件分配两个数据结构：<strong>索引节点（index node）和目录项（directory entry）</strong>，它们主要用来记录文件的元信息和目录层次结构。</p><ul><li>索引节点，也就是 <em>inode</em>，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也<strong>同样都会被存储在硬盘</strong>中，所以索引节点同样占用磁盘空间。<li>目录项，也就是 <em>dentry</em>，用来记录文件的名字、索引节点指针以及与其他目录项的层级关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，<strong>目录项是由内核维护</strong>的一个数据结构，不存放于磁盘，而是缓存在内存。</ul><p>注意，目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。</p><p>==目录项和目录是一个东西吗？==</p><p>它们不是一个东西，目录是个文件，持久化存储在磁盘，而目录项是内核一个数据结构，缓存在内存。</p><p>如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。</p><p>==那文件数据是如何存储在磁盘的呢？==</p><p>磁盘读写的最小单位是<strong>扇区</strong>，扇区的大小只有 <code class="language-plaintext highlighter-rouge">512B</code> 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。所以，文件系统把多个扇区组成了一个<strong>逻辑块</strong>，每次读写的最小单位就是逻辑块（数据块），Linux 中大小为 <code class="language-plaintext highlighter-rouge">4KB</code>，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。</p><p>目录项、索引结点、文件数据的关系图：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E9%A1%B9%E5%92%8C%E7%B4%A2%E5%BC%95%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="img" /></p><p>磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区：</p><ul><li><em>超级块</em>，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。<li><em>索引节点区</em>，用来存储索引节点；<li><em>数据块区</em>，用来存储文件或目录数据；</ul><h3 id="虚拟文件系统">虚拟文件系统</h3><p>文件系统的种类众多，而操作系统希望对用户提供一个统一的接口，于是在用户层与文件系统层引入了中间层，这个中间层就称为<strong>虚拟文件系统（Virtual File System，VFS）。</strong></p><p>VFS 定义了一组所有文件系统都支持的数据结构和标准接口，这样程序员不需要了解文件系统的工作原理，只需要了解 VFS 提供的统一接口即可。</p><p>文件系统首先要先挂载到某个目录才可以正常使用， Linux 系统在启动时，会把文件系统挂载到根目录。</p><h3 id="文件的使用">文件的使用</h3><p>操作系统会跟踪进程打开的所有文件，为每个进程维护一个打开文件表，文件表里的每一项代表「<strong>文件描述符</strong>」，所以说文件描述符是打开文件的标识。操作系统在打开文件表中维护着打开文件的状态和信息：</p><ul><li>文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；<li>文件打开计数器：因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，文件打开计数器跟踪打开和关闭的数量，当计数为 0 时，系统关闭文件，删除该条目；<li>文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；<li>访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求；</ul><p>文件系统的基本操作单位是数据块，读文件和写文件的过程为：</p><ul><li>当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。<li>当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。</ul><h3 id="文件的存储">文件的存储</h3><p>数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：</p><ul><li>连续空间存放方式<li>非连续空间存放方式</ul><p>其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。</p><p>==连续空间存放方式==</p><p>连续空间存放方式顾名思义，<strong>文件存放在磁盘「连续的」物理空间中</strong>。这种模式下，文件的数据都是紧密相连，<strong>读写效率很高</strong>，因为一次磁盘寻道就可以读出整个文件。</p><p>但使用连续存放的方式有一个条件，必须先知道文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。</p><p>缺点：磁盘空间碎片，文件长度不易扩展（扩展需要挪动到新空间，效率很低）。</p><p>==非连续空间存放方式==</p><p><strong>链式方式：</strong></p><p>链表的方式存放是离散的，不用连续的，于是就可以消除磁盘碎片，可大大提高磁盘空间的利用率，同时文件的长度可以动态扩展。根据实现的方式的不同，链表可分为「隐式链表」和「显式链接」两种形式。</p><p>文件要以「隐式链表」的方式存放的话，实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置，这样一个数据块连着一个数据块，从链头开始就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。</p><p>隐式链表的存放方式缺点：</p><ul><li>无法直接访问数据块，只能通过指针顺序访问文件<li>数据块指针会消耗一定的内存空间<li>稳定性较差，如果链表中的某个指针丢失或损坏，会导致文件数据的丢失。</ul><p>显式链接可以解决上述问题，把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中，该表在整个磁盘仅设置一张，每个表项中存放链接指针，指向下一个数据块号。</p><p>由于查找记录的过程是在内存中进行的，因而不仅显著地提高了检索速度，而且大大减少了访问磁盘的次数。但也正是整个表都存放在内存中的关系，它的主要的缺点是不适用于大磁盘。</p><p><strong>索引方式：</strong></p><p>索引的实现是为每个文件创建一个「<strong>索引数据块</strong>」，里面存放的是<strong>指向文件数据块的指针列表</strong>。另外，<strong>文件头需要包含指向「索引数据块」的指针</strong>，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。</p><p>索引的方式优点在于：</p><ul><li>文件的创建、增大、缩小很方便；<li>不会有碎片的问题；<li>支持顺序读写和随机读写；</ul><p>缺点就是存储索引带来的开销。</p><p>如果文件很大，大到一个索引数据块放不下索引信息，我们可以通过组合的方式，来处理大文件的存储。</p><p>先来看看<strong>链表 + 索引</strong>的组合，这种组合称为「链式索引块」，它的实现方式是在索引数据块留出一个存放下一个索引数据块的指针，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。</p><p>还有另外一种组合方式是<strong>索引 + 索引</strong>的方式，这种组合称为「多级索引块」，实现方式是通过一个索引块来存放多个索引数据块，一层套一层索引。</p><h3 id="空闲空间管理">空闲空间管理</h3><p>针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法：</p><ul><li>空闲表法<li>空闲链表法<li>位图法</ul><p>==空闲表法==</p><p>空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E8%A1%A8%E6%B3%95.png" alt="空闲表法" /></p><p>当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。</p><p>这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。</p><p>==空闲链表法==</p><p>我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。</p><p>当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。</p><p>特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。</p><p>==位图法==</p><p>位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。</p><p>在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。</p><h3 id="文件系统的结构">文件系统的结构</h3><p>Linux文件系统使用块组结构，有 N 多的块组，就能够表示 N 大的文件。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E7%BB%84.png" alt="img" /></p><p>最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：</p><ul><li><em>超级块</em>，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。<li><em>块组描述符</em>，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。<li><em>数据位图和 inode 位图</em>， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。<li><em>inode 列表</em>，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。<li><em>数据块</em>，包含文件的有用数据。</ul><p>每个块组里有很多重复的信息，比如<strong>超级块和块组描述符表，这两个都是全局信息，而且非常的重要</strong>，这么做是有两个原因：</p><ul><li>如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。<li>通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。</ul><h3 id="目录的存储">目录的存储</h3><p>和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。在目录文件的块中，最简单的保存格式就是<strong>列表</strong>，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E5%93%88%E5%B8%8C%E8%A1%A8.png" alt="目录格式哈希表" /></p><p>通常，第一项是「<code class="language-plaintext highlighter-rouge">.</code>」，表示当前目录，第二项是「<code class="language-plaintext highlighter-rouge">..</code>」，表示上一级目录，接下来就是一项一项的文件名和 inode。如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。于是，保存目录的格式改成<strong>哈希表</strong>，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。</p><p>目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。</p><h3 id="软链接和硬链接">软链接和硬链接</h3><p>有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过<strong>硬链接（Hard Link）</strong> 和<strong>软链接（Symbolic Link）</strong> 的方式来实现，它们都是比较特殊的文件，实现方式也是不相同的。</p><p>硬链接是<strong>多个目录项中的「索引节点」指向一个文件</strong>，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以<strong>硬链接是不可用于跨文件系统的</strong>。由于多个目录项都是指向一个 inode，那么<strong>只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。</strong></p><p>软链接相当于重新创建一个文件，这个文件有<strong>独立的 inode</strong>，但是这个<strong>文件的内容是另外一个文件的路径</strong>，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以<strong>软链接是可以跨文件系统的</strong>，甚至<strong>目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。</strong></p><h3 id="文件-io">文件 I/O</h3><h4 id="缓冲与非缓冲-io">缓冲与非缓冲 I/O</h4><p>文件操作的标准库是可以实现数据的缓存，那么<strong>根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O</strong>：</p><ul><li>缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。<li>非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。</ul><p>这里所说的「缓冲」特指标准库内部实现的缓冲。比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。</p><h4 id="直接与非直接-io">直接与非直接 I/O</h4><p><strong>根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O</strong>：</p><ul><li>直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。<li>非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。</ul><p>==如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？==</p><p>以下几种场景会触发内核缓存的数据写入磁盘：</p><ul><li>在调用 <code class="language-plaintext highlighter-rouge">write</code> 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；<li>用户主动调用 <code class="language-plaintext highlighter-rouge">sync</code>，内核缓存会刷到磁盘上；<li>当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；<li>内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；</ul><h4 id="阻塞与非阻塞-io-vs-同步与异步-io">阻塞与非阻塞 I/O VS 同步与异步 I/O</h4><p><strong>阻塞I/O</strong>：当用户程序执行 <code class="language-plaintext highlighter-rouge">read</code> ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，<code class="language-plaintext highlighter-rouge">read</code> 才会返回。阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png" alt="阻塞 I/O" /></p><p><strong>非阻塞I/O</strong>：非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，<code class="language-plaintext highlighter-rouge">read</code> 调用才可以获取到结果。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png" alt="非阻塞 I/O" /></p><p>这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。</p><p>应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。为了解决这个问题， <strong>I/O 多路复用</strong>技术就出来了，如 select、poll、epoll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。</p><p>下图是使用 select I/O 多路复用过程。注意，<code class="language-plaintext highlighter-rouge">read</code> 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个<strong>同步的过程</strong>，需要等待：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20%E7%9A%84%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8.png" alt="I/O 多路复用" /></p><p>实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用<strong>都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。</strong></p><p><strong>真正的异步 I/O</strong>是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。当我们发起 <code class="language-plaintext highlighter-rouge">aio_read</code> 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png" alt="异步 I/O" /></p><p>下面这张图，总结了以上几种 I/O 模型：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%90%8C%E6%AD%A5VS%E5%BC%82%E6%AD%A5IO.png" alt="img" /></p><p>在前面我们知道了，I/O 是分为两个过程的：</p><ol><li>数据准备的过程<li>数据从内核空间拷贝到用户进程缓冲区的过程</ol><p>阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。</p><p>异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。</p><p>==用故事去理解这几种 I/O 模型==</p><p>举个你去饭堂吃饭的例子，你好比用户程序，饭堂好比操作系统。</p><p>阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。</p><p>非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。</p><p>基于非阻塞的 I/O 多路复用好比，你去饭堂吃饭，发现有一排窗口，饭堂阿姨告诉你这些窗口都还没做好菜，等做好了再通知你，于是等啊等（<code class="language-plaintext highlighter-rouge">select</code> 调用中），过了一会阿姨通知你菜做好了，但是不知道哪个窗口的菜做好了，你自己看吧。于是你只能一个一个窗口去确认，后面发现 5 号窗口菜做好了，于是你让 5 号窗口的阿姨帮你打菜到饭盒里，这个打菜的过程你是要等待的，虽然时间不长。打完菜后，你自然就可以离开了。</p><p>异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。</p><h2 id="进程写文件时进程发生了崩溃已写入的数据会丢失吗">进程写文件时，进程发生了崩溃，已写入的数据会丢失吗？</h2><p>不会的。因为进程在执行 write （使用缓冲 IO）系统调用的时候，实际上是将文件数据写到了内核的 page cache，它是文件系统中用于缓存文件数据的缓冲，所以即使进程崩溃了，文件数据还是保留在内核的 page cache，我们读数据的时候，也是从内核的 page cache 读取，因此还是依然读的进程崩溃前写入的数据。内核会找个合适的时机，将 page cache 中的数据持久化到磁盘。但是如果 page cache 里的文件数据，在持久化到磁盘化到磁盘之前，系统发生了崩溃，那这部分数据就会丢失了。当然， 我们也可以在程序里调用 fsync 函数，在写文文件的时候，立刻将文件数据持久化到磁盘，这样就可以解决系统崩溃导致的文件数据丢失的问题。</p><p>==Page Cache 是什么？==</p><p>Page Cache 的本质是由 Linux 内核管理的内存区域，page 是内存管理分配的基本单位， Page Cache 由多个 page 构成。page 在操作系统中通常为 4KB 大小，而 Page Cache 的大小则为 4KB 的整数倍。</p><p>Page Cache 的优点：</p><ul><li>加快数据访问。如果数据能够在内存中进行缓存，那么下一次访问就不需要通过磁盘 I/O 了，直接命中内存缓存即可，而内存访问比磁盘访问快很多。<li>减少磁盘I/O次数。得益于 Page Cache 的缓存以及预读能力，而程序又往往符合局部性原理，因此通过一次 I/O 将多个 page 装入 Page Cache 能够减少磁盘 I/O 次数， 进而提高系统磁盘 I/O 吞吐量。</ul><p>Page Cache 的缺点：</p><ul><li>占用额外的物理内存空间，物理内存比较紧缺的时候可能会导致频繁的swap操作。<li>由于在内核层，所以很难在用户空间进行优化。</ul><h1 id="设备管理">设备管理</h1><h2 id="键盘敲入-a-字母时操作系统期间发生了什么">键盘敲入 A 字母时，操作系统期间发生了什么？</h2><h3 id="设备控制器">设备控制器</h3><p>为了屏蔽设备之间的差异，对各种输入输出设备做统一管理，每个设备都有一个叫设备控制器（Device Control） 的组件，比如硬盘有硬盘控制器、显示器有视频控制器等。</p><p>因为这些控制器都很清楚的知道对应设备的用法和功能，所以 CPU 是通过设备控制器来和设备打交道的。设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：</p><ul><li>通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或执行其他操作。<li>通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。</ul><p>实际上，控制器有三类寄存器，分别是状态寄存器（Status Register）、 命令寄存器（Command Register）以及数据寄存器（Data Register），作用如下：</p><ul><li><em>数据寄存器</em>，CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。<li><em>命令寄存器</em>，CPU 发送一个命令，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。<li><em>状态寄存器</em>，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。</ul><p>如果传输的数据量比较大，控制器会设立一个可读写的数据缓冲区：</p><ul><li>CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。<li>CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。</ul><p>这样做是为了，减少对设备的频繁操作，</p><p>CPU有两种方法与控制寄存器和数据缓冲区进行通信：</p><ul><li><p><em>端口 I/O</em>，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 <code class="language-plaintext highlighter-rouge">in/out</code> 类似的指令。</p><li><p><em>内存映射 I/O</em>，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。</p></ul><h3 id="io-控制方式">I/O 控制方式</h3><p>设备读写操作完成之后，如何通知CPU呢？一种是轮询，会占用大量占用CPU的时间，另一种就是中断，中断会打断CPU，对于频繁读取数据的操作并不友好，也会占用很多的时间。</p><p>对于这一类设备的解决方法就是使用<strong>DMA（Direct Memory Access）</strong>功能，它可以使设备在CPU不参与的情况下， 自行地把设备读取的数据放入到内存，需要有DMA控制器硬件的支持。工作方式如下：</p><ul><li>CPU 对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；<li>接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；<li>当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；<li>DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；</ul><h3 id="设备驱动程序">设备驱动程序</h3><p>虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使用模式都是不同的，所以为了屏蔽「设备控制器」的差异，引入了<strong>设备驱动程序</strong>。</p><p>设备控制器不属于操作系统范畴，它是属于硬件，而设备驱动程序属于操作系统的一部分，操作系统的内核代码可以像本地调用代码一样使用设备驱动程序的接口，而设备驱动程序是面向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。</p><p>不同的设备控制器虽然功能不同，但是<strong>设备驱动程序会提供统一的接口给操作系统</strong>，这样不同的设备驱动程序，就可以以相同的方式接入操作系统。</p><h3 id="通用块层">通用块层</h3><p>输入输出设备可以分为块设备和字符设备。对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的<strong>通用块层</strong>，来管理不同的块设备。</p><p>通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：</p><ul><li>第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；<li>第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。</ul><p>Linux 内存支持 5 种 I/O 调度算法，分别是：</p><ul><li>没有调度算法<li>先入先出调度算法<li>完全公平调度算法<li>优先级调度<li>最终期限调度算法</ul><p>第一种，没有调度算法，是的，你没听错，它不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。</p><p>第二种，先入先出调度算法，这是最简单的 I/O 调度算法，先进入 I/O 调度队列的 I/O 请求先发生。</p><p>第三种，完全公平调度算法，大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。</p><p>第四种，优先级调度算法，顾名思义，优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。</p><p>第五种，最终期限调度算法，分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。</p><h3 id="存储系统-io-软件分层">存储系统 I/O 软件分层</h3><p>可以把 Linux 存储系统的 I/O 由上到下可以分为三个层次，分别是文件系统层、通用块层、设备层。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png" alt="img" /></p><p>这三个层次的作用是：</p><ul><li>文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。<li>通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。<li>设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。</ul><p>有了文件系统接口之后，不但可以通过文件系统的命令行操作设备，也可以通过应用程序，调用 <code class="language-plaintext highlighter-rouge">read</code>、<code class="language-plaintext highlighter-rouge">write</code> 函数，就像读写文件一样操作设备，所以说设备在 Linux 下，也只是一个特殊的文件。</p><h3 id="键盘敲入字母时期间发生了什么">键盘敲入字母时，期间发生了什么？</h3><p>那当用户输入了键盘字符，<strong>键盘控制器</strong>就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送<strong>中断请求</strong>。</p><p>CPU 收到中断请求后，操作系统会<strong>保存被中断进程的 CPU 上下文</strong>，然后调用键盘的<strong>中断处理程序</strong>。</p><p>键盘的中断处理程序是在<strong>键盘驱动程序</strong>初始化时注册的，那键盘<strong>中断处理函数</strong>的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。</p><p>得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。</p><p>显示出结果后，<strong>恢复被中断进程的上下文</strong></p><h1 id="网络系统">网络系统</h1><h2 id="什么是零拷贝">什么是零拷贝？</h2><h3 id="为什么要有-dma-技术">为什么要有 DMA 技术?</h3><p>在没有 DMA 技术前，I/O 的过程是这样的：</p><ul><li>CPU 发出对应的指令给磁盘控制器，然后返回；<li>磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个<strong>中断</strong>；<li>CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。</ul><p>整个数据的传输，都需要CPU参与，如果传输大量数据的时候，会占用大量的CPU时间。于是就有了DMA（直接内存访问），在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。</p><p>使用DMA控制器进行数据传输的过程：</p><ul><li>用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；<li>操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；<li>DMA 进一步将 I/O 请求发送给磁盘；<li>磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；<li><strong>DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务</strong>；<li>当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；<li>CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；</ul><p>CPU只需要告诉DMA读取数据的请求以及读取到的数据放哪里就行了。</p><h3 id="传统的文件传输有多糟糕">传统的文件传输有多糟糕？</h3><p>如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。也就是一个read和一个write系统调用就可以了，但是过程很复杂：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img" /></p><p>首先，期间共<strong>发生了 4 次用户态与内核态的上下文切换</strong>，因为发生了两次系统调用，一次是 <code class="language-plaintext highlighter-rouge">read()</code> ，一次是 <code class="language-plaintext highlighter-rouge">write()</code>，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。</p><p>其次，还<strong>发生了 4 次数据拷贝</strong>，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程：</p><ul><li><em>第一次拷贝</em>，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。<li><em>第二次拷贝</em>，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。<li><em>第三次拷贝</em>，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。<li><em>第四次拷贝</em>，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。</ul><p>我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低系统性能。</p><p>所以，<strong>要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数</strong>。</p><h3 id="如何优化文件传输的性能">如何优化文件传输的性能？</h3><p>==先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？==</p><p>读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。</p><p>而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。</p><p>所以，<strong>要想减少上下文切换到次数，就要减少系统调用的次数</strong>。</p><p>==再来看看，如何减少「数据拷贝」的次数？==</p><p>在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。</p><p>因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此<strong>用户的缓冲区是没有必要存在的</strong>。</p><h3 id="如何实现零拷贝">如何实现零拷贝？</h3><p>零拷贝技术实现的方式通常有 2 种：</p><ul><li>mmap + write<li>sendfile</ul><p>==mmap + write==</p><p>在前面我们知道，<code class="language-plaintext highlighter-rouge">read()</code> 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 <code class="language-plaintext highlighter-rouge">mmap()</code> 替换 <code class="language-plaintext highlighter-rouge">read()</code> 系统调用函数。</p><p><code class="language-plaintext highlighter-rouge">mmap()</code> 系统调用函数会直接把内核缓冲区里的数据「<strong>映射</strong>」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" /></p><ul><li>应用进程调用了 <code class="language-plaintext highlighter-rouge">mmap()</code> 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；<li>应用进程再调用 <code class="language-plaintext highlighter-rouge">write()</code>，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；<li>最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。</ul><p>我们可以得知，通过使用 <code class="language-plaintext highlighter-rouge">mmap()</code> 来代替 <code class="language-plaintext highlighter-rouge">read()</code>， 可以减少一次数据拷贝的过程。</p><p>==sendfile==</p><p>统调用函数 <code class="language-plaintext highlighter-rouge">sendfile()</code>专门用来发送文件。</p><div class="language-c highlighter-rouge"><div class="code-header"> <span text-data=" C "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="cp">#include</span> <span class="cpf">&lt;sys/socket.h&gt;</span><span class="cp">
</span><span class="kt">ssize_t</span> <span class="nf">sendfile</span><span class="p">(</span><span class="kt">int</span> <span class="n">out_fd</span><span class="p">,</span> <span class="kt">int</span> <span class="n">in_fd</span><span class="p">,</span> <span class="kt">off_t</span> <span class="o">*</span><span class="n">offset</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">count</span><span class="p">);</span>
</pre></table></code></div></div><p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。</p><p>首先，它可以替代前面的 <code class="language-plaintext highlighter-rouge">read()</code> 和 <code class="language-plaintext highlighter-rouge">write()</code> 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。</p><p>其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img" /></p><p>但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（<em>The Scatter-Gather Direct Memory Access</em>）技术（和普通的 DMA 有所不同），<code class="language-plaintext highlighter-rouge">sendfile()</code> 系统调用的过程发生了点变化，具体过程如下：</p><ul><li>第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；<li>第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；</ul><p>所以，这个过程之中，只进行了 2 次数据拷贝，如下图：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" /></p><p>这就是所谓的<strong>零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</strong></p><p>零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，<strong>只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。</strong></p><p>所以，总体来看，<strong>零拷贝技术可以把文件传输的性能提高至少一倍以上</strong></p><h3 id="pagecache-有什么作用">PageCache 有什么作用？</h3><p>把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是<strong>磁盘高速缓存（PageCache）</strong>。</p><p>ageCache 的优点主要是两个：</p><ul><li>缓存最近被访问的数据；<li>预读功能；（读取数据页的时候，根据局部性原理，会预读邻近的数据页）</ul><p>这两个做法，将大大提高读写磁盘的性能。</p><p><strong>但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用</strong></p><p>这是因为在传输大文件时，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满，这样会带来两个问题：</p><ul><li>PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了，在高并发的环境下可能会带来严重的性能问题。<li>PageCache 中的大文件数据，可能某些部分的文件数据被再次访问的概率比较低，也就相当于没有享受到缓存带来的好处，但这样却耗费 DMA 多拷贝到 PageCache 一次；</ul><p>所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术。</p><h3 id="大文件传输用什么方式实现">大文件传输用什么方式实现？</h3><p>大文件传输不应该使用PageCache，异步I/O可以解决I/O请求的阻塞问题，同时不涉及PageCache。使用 PageCache 的 I/O 则叫缓存 I/O，而绕开 PageCache 的 I/O 叫直接 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。<strong>所以，在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术，这样就可以无阻塞地读取文件了。</strong></p><p>直接 I/O 应用场景常见的两种：</p><ul><li>应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。<li>传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。</ul><p>所以，传输文件的时候，我们要根据文件的大小来使用不同的方式：</p><ul><li>传输大文件的时候，使用「异步 I/O + 直接 I/O」；<li>传输小文件的时候，则使用「零拷贝技术」；</ul><h2 id="io-多路复用selectpollepoll">I/O 多路复用：select/poll/epoll</h2><h3 id="最基本的-socket-模型">最基本的 Socket 模型</h3><p>服务端首先调用 <code class="language-plaintext highlighter-rouge">socket()</code> 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调用 <code class="language-plaintext highlighter-rouge">bind()</code> 函数，给这个 Socket 绑定一个 <strong>IP 地址和端口</strong>，绑定这两个的目的是什么？</p><ul><li>绑定端口的目的：当内核收到 TCP 报文，通过 TCP 头里面的端口号，来找到我们的应用程序，然后把数据传递给我们。<li>绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址，当绑定一个网卡时，内核在收到该网卡上的包，才会发给我们；</ul><p>绑定完 IP 地址和端口后，就可以调用 <code class="language-plaintext highlighter-rouge">listen()</code> 函数进行监听，此时对应 TCP 状态图中的 <code class="language-plaintext highlighter-rouge">listen</code>，如果要判定服务器中一个网络程序有没有启动，可以通过 <code class="language-plaintext highlighter-rouge">netstat</code> 命令查看对应的端口号是否有被监听。</p><p>服务端进入了监听状态后，通过调用 <code class="language-plaintext highlighter-rouge">accept()</code> 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。</p><p>那客户端是怎么发起连接的呢？客户端在创建好 Socket 后，调用 <code class="language-plaintext highlighter-rouge">connect()</code> 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。</p><p>在 TCP 连接的过程中，服务器的内核实际上为每个 Socket 维护了两个队列：</p><ul><li>一个是还没完全建立连接的队列，称为 <strong>TCP 半连接队列</strong>，这个队列都是没有完成三次握手的连接，此时服务端处于 <code class="language-plaintext highlighter-rouge">syn_rcvd</code> 的状态；<li>一个是已经建立连接的队列，称为 <strong>TCP 全连接队列</strong>，这个队列都是完成了三次握手的连接，此时服务端处于 <code class="language-plaintext highlighter-rouge">established</code> 状态；</ul><p>当 TCP 全连接队列不为空后，服务端的 <code class="language-plaintext highlighter-rouge">accept()</code> 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。</p><p>注意，监听的 Socket 和真正用来传数据的 Socket 是两个：</p><ul><li>一个叫作<strong>监听 Socket</strong>；<li>一个叫作<strong>已连接 Socket</strong>；</ul><p>连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 <code class="language-plaintext highlighter-rouge">read()</code> 和 <code class="language-plaintext highlighter-rouge">write()</code> 函数来读写数据。至此， TCP 协议的 Socket 程序的调用过程就结束了，整个过程如下图：</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/tcp_socket.png" alt="img" /></p><h3 id="如何服务更多的用户">如何服务更多的用户？</h3><p>前面提到的 TCP Socket 调用流程是最简单、最基本的，它基本只能一对一通信，因为使用的是<strong>同步阻塞</strong>的方式，当服务端在还没处理完一个客户端的网络 I/O 时，或者 读写操作发生阻塞时，其他客户端是无法与服务端连接的。</p><p>务器单机理论最大能连接多少个客户端？TCP 连接是由四元组唯一确认的，这个四元组就是：<strong>本机IP, 本机端口, 对端IP, 对端端口</strong>。服务器作为服务方，通常会在本地固定监听一个端口，等待客户端的连接。因此服务器的本地 IP 和端口是固定的，于是对于服务端 TCP 连接的四元组只有对端 IP 和端口是会变化的，所以最大 TCP 连接数 = 客户端 IP 数×客户端端口数。对于 IPv4，客户端的 IP 数最多为 2 的 32 次方，客户端的端口数最多为 2 的 16 次方，也就是<strong>服务端单机最大 TCP 连接数约为 2 的 48 次方</strong>。但是服务器肯定承载不了那么大的连接数，主要会受两个方面的限制：</p><ul><li><strong>文件描述符</strong>，Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；<li><strong>系统内存</strong>，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；</ul><p>从硬件资源角度看，对于 2GB 内存千兆网卡的服务器，如果每个请求处理占用不到 200KB 的内存和 100Kbit 的网络带宽就可以满足并发 1 万个请求。当然，这与服务器的网络I/O模型也有很大关系。</p><h3 id="多进程模型">多进程模型</h3><p>如果服务器要支持多个客户端，其中比较传统的方式，就是使用<strong>多进程模型</strong>，也就是为每个客户端分配一个进程来处理请求。服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 <code class="language-plaintext highlighter-rouge">fork()</code> 函数创建一个子进程，实际上就把父进程所有相关的东西都<strong>复制</strong>一份，包括文件描述符、内存地址空间、程序计数器、执行的代码等。</p><p>这两个进程刚复制完的时候，几乎一摸一样。不过，会根据<strong>返回值</strong>来区分是父进程还是子进程，如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。正因为子进程会<strong>复制父进程的文件描述符</strong>，于是就可以直接使用「已连接 Socket 」和客户端通信了。</p><p>可以发现，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E5%A4%9A%E8%BF%9B%E7%A8%8B.png" alt="img" /></p><p>另外，当「子进程」退出时，实际上内核里还会保留该进程的一些信息，也是会占用内存的，如果不做好“回收”工作，就会变成<strong>僵尸进程</strong>，随着僵尸进程越多，会慢慢耗尽我们的系统资源。因此，父进程要“善后”好自己的孩子，怎么善后呢？那么有两种方式可以在子进程退出后回收资源，分别是调用 <code class="language-plaintext highlighter-rouge">wait()</code> 和 <code class="language-plaintext highlighter-rouge">waitpid()</code> 函数。</p><p>这种用多个进程来应付多个客户端的方式，在应对 100 个客户端还是可行的，但是当客户端数量高达一万时，肯定扛不住的，<strong>因为每产生一个进程，必会占据一定的系统资源，而且进程间上下文切换的“包袱”是很重的，性能会大打折扣</strong>。</p><h3 id="多线程模型">多线程模型</h3><p>当服务器与客户端 TCP 完成连接后，通过 <code class="language-plaintext highlighter-rouge">pthread_create()</code> 函数创建线程，然后将「已连接 Socket」的文件描述符传递给线程函数，接着在线程里和客户端进行通信，从而达到并发处理的目的。</p><p>如果每来一个连接就创建一个线程，线程运行完后，还得操作系统还得销毁线程，虽说线程切换的上写文开销不大，但是如果频繁创建和销毁线程，系统开销也是不小的。那么，我们可以使用<strong>线程池</strong>的方式来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若干个线程，这样当由新连接建立时，将这个已连接的 Socket 放入到一个队列里，然后线程池里的线程负责从队列中取出已连接 Socket 进程处理。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/%E7%BA%BF%E7%A8%8B%E6%B1%A0.png" alt="img" /></p><p>需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要<strong>加锁</strong>。上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程，那么如果要达到 C10K，意味着要一台机器维护 1 万个连接，相当于要维护 1 万个进程/线程，操作系统就算死扛也是扛不住的。</p><h3 id="io-多路复用">I/O 多路复用</h3><p>既然为每个请求分配一个进程/线程的方式不合适，那有没有可能只使用一个进程来维护多个 Socket 呢？答案是有的，那就是 <strong>I/O 多路复用</strong>技术。</p><p>一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这就是多路复用，这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用。</p><p>我们熟悉的 select/poll/epoll 内核提供给用户态的多路复用系统调用，<strong>进程可以通过一个系统调用函数从内核中获取多个事件</strong>。</p><p>select/poll/epoll 是如何获取网络事件的呢？在获取事件时，先把所有连接（文件描述符）传给内核，再由内核返回产生了事件的连接，然后在用户态中再处理这些连接对应的请求即可。</p><h3 id="selectpoll">select/poll</h3><p>select 实现多路复用的方式是，将已连接的 Socket 都放到一个<strong>文件描述符集合</strong>，然后调用 select 函数将文件描述符集合<strong>拷贝</strong>到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过<strong>遍历</strong>文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合<strong>拷贝</strong>回用户态里，然后用户态还需要再通过<strong>遍历</strong>的方法找到可读或可写的 Socket，然后再对其处理。</p><p>所以，对于 select 这种方式，需要进行 <strong>2 次「遍历」文件描述符集合</strong>，一次是在内核态里，一个次是在用户态里 ，而且还会发生 <strong>2 次「拷贝」文件描述符集合</strong>，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。</p><p>select 使用固定长度的 BitsMap，表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最大值为 <code class="language-plaintext highlighter-rouge">1024</code>，只能监听 0~1023 的文件描述符。</p><p>poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。</p><p>但是 poll 和 select 并没有太大的本质区别，<strong>都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合</strong>，这种方式随着并发数上来，性能的损耗会呈指数级增长。</p><h3 id="epoll">epoll</h3><p>先复习下 epoll 的用法。如下的代码中，先用e poll_create 创建一个 epol l对象 epfd，再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。</p><p><em>第一点</em>，epoll 在内核里使用<strong>红黑树来跟踪进程所有待检测的文件描述字</strong>，把需要监控的 socket 通过 <code class="language-plaintext highlighter-rouge">epoll_ctl()</code> 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 <code class="language-plaintext highlighter-rouge">O(logn)</code>。而 select/poll 内核里没有类似 epoll 红黑树这种保存所有待检测的 socket 的数据结构，所以 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。</p><p><em>第二点</em>， epoll 使用<strong>事件驱动</strong>的机制，内核里<strong>维护了一个链表来记录就绪事件</strong>，当某个 socket 有事件发生时，通过<strong>回调函数</strong>内核会将其加入到这个就绪事件列表中，当用户调用 <code class="language-plaintext highlighter-rouge">epoll_wait()</code> 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png" alt="img" /></p><p>epoll 的方式即使监听的 Socket 数量越多的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了，上限就为系统定义的进程打开的最大文件描述符个数。因而，<strong>epoll 被称为解决 C10K 问题的利器</strong>。</p><h3 id="边缘触发和水平触发">边缘触发和水平触发</h3><p>epoll 支持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和水平触发（level-triggered，LT）。它们的区别：</p><ul><li>使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，<strong>服务器端只会从 epoll_wait 中苏醒一次</strong>，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，因此我们程序要保证一次性将内核缓冲区的数据读取完；<li>使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，<strong>服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束</strong>，目的是告诉我们有数据需要读取；</ul><p>一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。</p><p>select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。另外，使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用（因为多路复用 API 返回的事件并不一定可读写的，如果使用阻塞 I/O， 那么在调用 read/write 时则会发生程序阻塞）。</p><h2 id="高性能网络模式reactor-和-proactor">高性能网络模式：Reactor 和 Proactor</h2><h3 id="reactor">Reactor</h3><p>基于面向对象的思想，对I/O多路复用作了一层封装，让使用者用考虑底层网络 API 的细节，只需要关注应用代码的编写，即Reactor模式。</p><p>==单 Reactor 单进程 / 线程==</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="img" /></p><p>可以看到进程里有 <strong>Reactor、Acceptor、Handler</strong> 这三个对象：</p><ul><li>Reactor 对象的作用是监听和分发事件；<li>Acceptor 对象的作用是获取连接；<li>Handler 对象的作用是处理业务；</ul><p>「单 Reactor 单进程」方案流程：</p><ul><li>Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；<li>如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；<li>如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；<li>Handler 对象通过 read -&gt; 业务处理 -&gt; send 的流程来完成完整的业务流程。</ul><p>全部工作在单进程中完成，实现起来比较简单，但是，这种方案存在 2 个缺点：</p><ul><li>第一个缺点，因为只有一个进程，<strong>无法充分利用 多核 CPU 的性能</strong>；<li>第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，<strong>如果业务处理耗时比较长，那么就造成响应的延迟</strong>；</ul><p>所以，单 Reactor 单进程的方案<strong>不适用计算机密集型的场景，只适用于业务处理非常快速的场景</strong>。</p><p>==单 Reactor 多线程 / 多进程==</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" /></p><ul><li>Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；<li>如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；<li><p>如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；</p><li>Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；<li>子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；</ul><p>单 Reator 多线程的方案优势在于<strong>能够充分利用多核 CPU 的能</strong>，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。要避免多线程由于竞争共享资源而导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。</p><p><strong>因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方</strong>。</p><p>==多 Reactor 多进程 / 线程==</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" /></p><ul><li>主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；<li>子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。<li>如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。<li>Handler 对象通过 read -&gt; 业务处理 -&gt; send 的流程来完成完整的业务流程。</ul><p>多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：</p><ul><li>主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。<li>主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。</ul><h3 id="proactor">Proactor</h3><p>Proactor 采用了异步 I/O 技术，所以被称为异步网络模型。</p><p>当我们发起 <code class="language-plaintext highlighter-rouge">aio_read</code> （异步 I/O） 之后，就立即返回，内核准备数据并且自动将数据从内核空间拷贝到用户空间，整个过程都是异步的，内核自动完成的，和前面的同步操作不一样，<strong>应用程序并不需要主动发起拷贝动作</strong>。</p><p>Reactor 和 Proactor 的区别：</p><ul><li><strong>Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件</strong>。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。<li><strong>Proactor 是异步网络模式， 感知的是已完成的读写事件</strong>。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。</ul><p>因此，<strong>Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」</strong>，而 <strong>Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」</strong>。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。</p><p><img data-proofer-ignore data-src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/Proactor.png" alt="img" /></p><p>Proactor 模式的工作流程：</p><ul><li>Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；<li>Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；<li>Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；<li>Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；<li>Handler 完成业务处理；</ul><p>在 Linux 下的异步 I/O 是不完善的， <code class="language-plaintext highlighter-rouge">aio</code> 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案</p><h2 id="什么是一致性哈希">什么是一致性哈希？</h2><h3 id="如何分配请求">如何分配请求？</h3><p>服务器集群是如何分配客户端的请求的？这个问题就是负载均衡问题。</p><p>最简单的方式就是引入一个中间的负载均衡层，让它将外界的请求轮流的转发给内部的集群，达到分配请求的目的。考虑到每个服务器的配置不同，可以引入权重值，做加权轮训。</p><p>轮询的使用场景是每个服务器存储的数据都是相同的，所以访问任意一个数据库都能得到结果。</p><p>但是在分布式系统中，每个节点存储的数据是不同的，比如<strong>一个分布式 KV（key-valu） 缓存系统，某个 key 应该到哪个或者哪些节点上获得，应该是确定的</strong>，不是说任意访问一个节点都可以得到缓存结果的。就需要使用别的负载均衡算法。</p><h3 id="使用哈希算法有什么问题">使用哈希算法有什么问题？</h3><p>哈希算法最简单的做法就是进行取模运算，比如分布式系统中有 3 个节点，基于 <code class="language-plaintext highlighter-rouge">hash(key) % 3</code> 公式对数据进行了映射。但是有一个很致命的问题，<strong>如果节点数量发生了变化，也就是在对系统做扩容或者缩容时，必须迁移改变了映射关系的数据</strong>，否则会出现查询不到数据的问题。</p><h3 id="使用一致性哈希算法有什么问题">使用一致性哈希算法有什么问题？</h3><p>一致性哈希算法就很好地解决了分布式系统在扩容或者缩容时，发生过多的数据迁移的问题。</p><p>一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而<strong>一致哈希算法是对 2^32 进行取模运算，是一个固定的值</strong>。我们可以把一致哈希算法是对 2^32 进行取模运算的结果值组织成一个圆环，即哈希环。</p><p><img data-proofer-ignore data-src="https://img-blog.csdnimg.cn/img_convert/0ea3960fef48d4cbaeb4bec4345301e7.png" alt="img" /></p><p>一致性哈希要进行两步哈希：</p><ul><li>第一步：对存储节点做哈希映射，比如根据节点的 IP 地址进行哈希；<li>第二步：当对数据进行存储或访问时，对数据进行哈希映射；</ul><p>所以，<strong>一致性哈希是指将「存储节点」和「数据」都映射到一个首尾相连的哈希环上</strong>。</p><p>问题来了，对「数据」进行哈希映射得到一个结果要怎么找到存储该数据的节点呢？</p><p>答案是，映射的结果值往<strong>顺时针的方向的找到第一个节点</strong>，就是存储该数据的节点。</p><p>所以，当需要对指定 key 的值进行读写的时候，要通过下面 2 步进行寻址：</p><ul><li>首先，对 key 进行哈希计算，确定此 key 在环上的位置；<li>然后，从这个位置沿着顺时针方向走，遇到的第一节点就是存储 key 的节点。</ul><p><strong>在一致哈希算法中，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响</strong>。但是<strong>一致性哈希算法并不保证节点能够在哈希环上分布均匀</strong>，这样就会带来一个问题，会有大量的请求集中在一个节点上。</p><p>所以，<strong>一致性哈希算法虽然减少了数据迁移量，但是存在节点分布不均匀的问题</strong>。</p><h3 id="如何通过虚拟节点提高均衡度">如何通过虚拟节点提高均衡度？</h3><p>要想解决节点能在哈希环上分配不均匀的问题，就是要有大量的节点，节点数越多，哈希环上的节点分布的就越均匀。</p><p>但问题是，实际中我们没有那么多节点。所以这个时候我们就加入<strong>虚拟节点</strong>，也就是对一个真实节点做多个副本。具体做法是，<strong>不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。</strong></p><p>另外，虚拟节点除了会提高节点的均衡度，还会提高系统的稳定性。<strong>当节点变化时，会有不同的节点共同分担系统的变化，因此稳定性更高</strong>。</p><p>比如，当某个节点被移除时，对应该节点的多个虚拟节点均会移除，而这些虚拟节点按顺时针方向的下一个虚拟节点，可能会对应不同的真实节点，即这些不同的真实节点共同分担了节点变化导致的压力。</p><p><strong>带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景</strong>。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AC%94%E8%AE%B0/'>笔记</a>, <a href='/categories/os/'>OS</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E6%80%BB%E7%BB%93/" class="post-tag no-text-decoration" >总结</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" class="post-tag no-text-decoration" >操作系统</a> <a href="/tags/%E5%85%AB%E8%82%A1/" class="post-tag no-text-decoration" >八股</a> <a href="/tags/os/" class="post-tag no-text-decoration" >OS</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=OS相关知识整理 - ShenshenZhou&url=https://shenshenzhou.github.io/posts/OS-knowledge/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=OS相关知识整理 - ShenshenZhou&u=https://shenshenzhou.github.io/posts/OS-knowledge/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=OS相关知识整理 - ShenshenZhou&url=https://shenshenzhou.github.io/posts/OS-knowledge/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/webserver-webbench/">Webserver压力测试</a><li><a href="/posts/Sort/">C++实现十大排序算法</a><li><a href="/posts/Redis-knowledge/">Redis相关知识整理</a><li><a href="/posts/MySQL-knowledge/">MySQL相关知识整理</a><li><a href="/posts/leetcode200/">LeetCode200题记录</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a> <a class="post-tag" href="/tags/%E6%80%BB%E7%BB%93/">总结</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/c-primer5/">C++Primer5</a> <a class="post-tag" href="/tags/%E7%AE%97%E6%B3%95/">算法</a> <a class="post-tag" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/">蓝桥杯</a> <a class="post-tag" href="/tags/%E9%A2%98%E8%A7%A3/">题解</a> <a class="post-tag" href="/tags/%E5%85%AB%E8%82%A1/">八股</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/%E7%BD%91%E9%A1%B5%E8%B5%84%E6%96%99/">网页资料</a></div></div></div><script src="https://fastly.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">文章内容</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/MySQL-knowledge/"><div class="card-body"> <span class="timeago small" >2022-07-15<i class="unloaded">2022-07-15T22:41:27+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MySQL相关知识整理</h3><div class="text-muted small"><p> 基础 执行一条select语句，期间发生了什么？ 首先需要了解MySQL的内部架构，共分为两层：Server层和存储引擎层 Server层负责建立连接、分析和执行SQL，主要包络连接器、查询缓存、解析器、预处理器、优化器、执行器等核心功能模块。 存储引擎层负责数据的存储和提取。MySQL支持MyISAM、InnoDB、Memory等多个存储引擎，不同的存储引擎公用一个Serv...</p></div></div></a></div><div class="card"> <a href="/posts/Redis-knowledge/"><div class="card-body"> <span class="timeago small" >2022-07-16<i class="unloaded">2022-07-16T22:35:05+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Redis相关知识整理</h3><div class="text-muted small"><p> 认识Redis 什么是Redis Redis 是一种基于内存的数据库，对数据的读写操作都是在内存中完成，因此读写速度非常快，常用于缓存，消息队列、分布式锁等场景。 Redis 提供了多种数据类型来支持不同的业务场景并且对数据类型的操作都是原子性的，因为执行命令由单线程负责的，不存在并发竞争的问题。 除此之外，Redis 还支持事务 、持久化、Lua 脚本、多种集群方案（主从复制模式、...</p></div></div></a></div><div class="card"> <a href="/posts/NetWork-knowledge/"><div class="card-body"> <span class="timeago small" >2022-07-19<i class="unloaded">2022-07-19T20:06:20+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>计网相关知识整理</h3><div class="text-muted small"><p> 基础 TCP/IP 网络模型有哪几层？ 首先，需要明确，为什么要有TCP/IP网络模型？ 因为如果要对同一台设备上的进程间进行通信，有很多种方式，比如管道、消息队列、共享内存、信号等，但是对不同设备上的进程间通信，就需要网络通信，而设备是多种多样的，所以就要兼容这些设备，就协商出了一套通用的网络协议。 网络协议是分层的，每一层都有各自的作用和职责。 应用层 向用户提供应用服务并规...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/NetWork-knowledge/" class="btn btn-outline-primary" prompt="上一篇"><p>计网相关知识整理</p></a> <a href="/posts/Singleton/" class="btn btn-outline-primary" prompt="下一篇"><p>设计模式之单例模式</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/ShenshenZhou">ShenshenZhou</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a> <a class="post-tag" href="/tags/%E6%80%BB%E7%BB%93/">总结</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/c-primer5/">C++Primer5</a> <a class="post-tag" href="/tags/%E7%AE%97%E6%B3%95/">算法</a> <a class="post-tag" href="/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/">蓝桥杯</a> <a class="post-tag" href="/tags/%E9%A2%98%E8%A7%A3/">题解</a> <a class="post-tag" href="/tags/%E5%85%AB%E8%82%A1/">八股</a> <a class="post-tag" href="/tags/linux/">Linux</a> <a class="post-tag" href="/tags/%E7%BD%91%E9%A1%B5%E8%B5%84%E6%96%99/">网页资料</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://fastly.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://fastly.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://fastly.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
